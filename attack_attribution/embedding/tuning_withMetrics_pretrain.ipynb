{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Introduction to the Keras Tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/keras_tuner\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/keras_tuner.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/keras_tuner.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/keras/keras_tuner.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The Keras Tuner is a library that helps you pick the optimal set of hyperparameters for your TensorFlow program. The process of selecting the right set of hyperparameters for your machine learning (ML) application is called *hyperparameter tuning* or *hypertuning*.\n",
        "\n",
        "Hyperparameters are the variables that govern the training process and the topology of an ML model. These variables remain constant over the training process and directly impact the performance of your ML program. Hyperparameters are of two types:\n",
        "1. **Model hyperparameters** which influence model selection such as the number and width of hidden layers\n",
        "2. **Algorithm hyperparameters** which influence the speed and quality of the learning algorithm such as the learning rate for Stochastic Gradient Descent (SGD) and the number of nearest neighbors for a k Nearest Neighbors (KNN) classifier\n",
        "\n",
        "In this tutorial, you will use the Keras Tuner to perform hypertuning for an image classification application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import TextVectorization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g83Lwsy-Aq2_"
      },
      "source": [
        "Install and import the Keras Tuner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hpMLpbt9jcO6",
        "outputId": "1cce9cd5-9be5-4dff-b6fd-d00eda6888ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 135 kB 5.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 33.5 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_leAIdFKAxAD"
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReV_UXOgCZvx"
      },
      "source": [
        "## Download and prepare the dataset\n",
        "\n",
        "In this tutorial, you will use the Keras Tuner to find the best hyperparameters for a machine learning model that classifies images of clothing from the [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HljH_ENLEdHa"
      },
      "source": [
        "Load the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OHlHs9Wj_PUM",
        "outputId": "4d7039a1-9e6b-487d-bd84-a470200771a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/eyalmazuz/ThreatIntelligenceCorpus/archive/refs/heads/master.zip\n",
            "39100416/Unknown - 4s 0us/stepDownloading data from https://github.com/eyalmazuz/AttackAttributionDataset/archive/refs/heads/master.zip\n",
            " 835584/Unknown - 0s 0us/step"
          ]
        }
      ],
      "source": [
        "# Pretraining\n",
        "url = 'https://github.com/eyalmazuz/ThreatIntelligenceCorpus/archive/refs/heads/master.zip' \n",
        "dataset_pretrain = tf.keras.utils.get_file('master.zip', url, extract=True, cache_dir='.', cache_subdir='')\n",
        "\n",
        "# Training Dataset\n",
        "url = 'https://github.com/eyalmazuz/AttackAttributionDataset/archive/refs/heads/master.zip' \n",
        "dataset = tf.keras.utils.get_file('master(1).zip', url,extract=True, cache_dir='.',cache_subdir='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bLVhXs3xrUD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38c3e568-6466-47ce-cab7-561dfd1df090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20560 files belonging to 1 classes.\n",
            "Found 238 files belonging to 12 classes.\n",
            "Using 191 files for training.\n",
            "Found 238 files belonging to 12 classes.\n",
            "Using 47 files for validation.\n"
          ]
        }
      ],
      "source": [
        "# Normalize pixel values between 0 and 1\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 5\n",
        "seed = 42\n",
        "\n",
        "pretrain_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'ThreatIntelligenceCorpus-master')\n",
        "\n",
        "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'AttackAttributionDataset-master',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "remaining_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'AttackAttributionDataset-master',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)\n",
        "\n",
        "val_ds = remaining_ds.take(int(len(remaining_ds)*0.5))\n",
        "\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_ds = remaining_ds.skip(len(val_ds))\n",
        "\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text preprocessing\n",
        "Next, define the dataset preprocessing steps required for your sentiment classification model. Initialize a TextVectorization layer with the desired parameters to vectorize movie reviews. "
      ],
      "metadata": {
        "id": "WnVZRo27eo2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "\n",
        "# Vocabulary size and number of words in a sequence.\n",
        "vocab_size = 100000\n",
        "sequence_length = 1000\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Note that the layer uses the custom standardization defined above.\n",
        "# Set maximum_sequence length as all samples are not of the same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_ds = pretrain_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "metadata": {
        "id": "_5RrIMMCeme_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5YEL2H2Ax3e"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "When you build a model for hypertuning, you also define the hyperparameter search space in addition to the model architecture. The model you set up for hypertuning is called a *hypermodel*.\n",
        "\n",
        "You can define a hypermodel through two approaches:\n",
        "\n",
        "* By using a model builder function\n",
        "* By subclassing the `HyperModel` class of the Keras Tuner API\n",
        "\n",
        "You can also use two pre-defined [HyperModel](https://keras.io/api/keras_tuner/hypermodels/) classes - [HyperXception](https://keras.io/api/keras_tuner/hypermodels/hyper_xception/) and [HyperResNet](https://keras.io/api/keras_tuner/hypermodels/hyper_resnet/) for computer vision applications.\n",
        "\n",
        "In this tutorial, you use a model builder function to define the image classification model. The model builder function returns a compiled model and uses hyperparameters you define inline to hypertune the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZQKodC-jtsva"
      },
      "outputs": [],
      "source": [
        "min_embedding_dim=16\n",
        "max_embedding_dim=1024\n",
        "\n",
        "\n",
        "def model_builder(hp):\n",
        "  model = keras.Sequential()\n",
        "  model.add(vectorize_layer)\n",
        "  hp_units_embed = hp.Int('units_emb_1', min_value=min_embedding_dim, max_value=max_embedding_dim, step=32)\n",
        "  model.add(Embedding(vocab_size,output_dim= hp_units_embed, name=\"embedding\"))\n",
        "  model.add(GlobalAveragePooling1D())\n",
        " \n",
        "  # # Tune the number of layers. Best to do it manually for now\n",
        "  # for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
        "  #     model.add(\n",
        "  #         layers.Dense(\n",
        "  #             # Tune number of units separately.\n",
        "  #             units=hp.Int(f\"units_{i}\", min_value=382, max_value=512, step=32),\n",
        "  #             activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
        "  #         )\n",
        "  #     )\n",
        "\n",
        "\n",
        "  # Tune the number of units in the first Dense layer + the activation function\n",
        "  # Choose an optimal value between 32-512\n",
        "  hp_units_1 = hp.Int('units_dense_1', min_value=200, max_value=1024, step=20)\n",
        "  hp_units_2 = hp.Int('units_dense_2', min_value=58, max_value=512, step=10)\n",
        "  hp_units_3 = hp.Int('units_dense_3', min_value=32, max_value=192, step=10)\n",
        "  hp_drop_1 = hp.Choice('units_drop_1', values=[0.01, 0.05, 0.1,0.15, 0.2, 0.25, 0.3])\n",
        "  hp_drop_2 = hp.Choice('units_drop_2', values=[0.01, 0.05, 0.1,0.15, 0.2, 0.25, 0.3])\n",
        "\n",
        "\n",
        "  model.add(keras.layers.Dense(units=hp_units_1, activation= hp.Choice(\"activation_1\", [\"relu\", \"tanh\"])))\n",
        "  model.add(Dense(units=hp_units_2, activation=hp.Choice(\"activation_2\", [\"relu\", \"tanh\"])))\n",
        "  model.add(tf.keras.layers.Dropout(hp_drop_1)) \n",
        "  model.add(Dense(units=hp_units_3, activation=hp.Choice(\"activation_3\", [\"relu\", \"tanh\"])))\n",
        "  model.add(tf.keras.layers.Dropout(hp_drop_2))\n",
        "  model.add(Dense(16, activation=hp.Choice(\"activation_4\", [\"relu\", \"tanh\"])))\n",
        "  model.add(Dense(len(class_names)))\n",
        "\n",
        "  # Tune the learning rate for the optimizer\n",
        "  # Choose an optimal value from 0.01, 0.005,  0.0011, 0.001,0.0009, 0.0005, 0.0001\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 5e-3, 1.1e-3, 9e-4, 5e-4, 1e-4])\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J1VYw4q3x0b"
      },
      "source": [
        "## Instantiate the tuner and perform hypertuning\n",
        "\n",
        "Instantiate the tuner to perform the hypertuning. The Keras Tuner has four tuners available - `RandomSearch`, `Hyperband`, `BayesianOptimization`, and `Sklearn`. In this tutorial, you use the [Hyperband](https://arxiv.org/pdf/1603.06560.pdf) tuner.\n",
        "\n",
        "To instantiate the Hyperband tuner, you must specify the hypermodel, the `objective` to optimize and the maximum number of epochs to train (`max_epochs`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oichQFly6Y46"
      },
      "outputs": [],
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=15,\n",
        "                     factor=3,\n",
        "                     directory='my_dir',\n",
        "                     project_name='intro_to_kt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaIhhdKf9VtI"
      },
      "source": [
        "The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. Hyperband determines the number of models to train in a bracket by computing 1 + log<sub>`factor`</sub>(`max_epochs`) and rounding it up to the nearest integer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwhBdXx0Ekj8"
      },
      "source": [
        "Create a callback to stop training early after reaching a certain value for the validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WT9IkS9NEjLc"
      },
      "outputs": [],
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "stop_early_val_accuracy = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKghEo15Tduy"
      },
      "source": [
        "Run the hyperparameter search. The arguments for the search method are the same as those used for `tf.keras.model.fit` in addition to the callback above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dSBQcTHF9cKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6368f6c0-3a2c-42f2-e4aa-95dcd5a51b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 00m 19s]\n",
            "val_accuracy: 0.6000000238418579\n",
            "\n",
            "Best val_accuracy So Far: 0.6399999856948853\n",
            "Total elapsed time: 00h 04m 23s\n",
            "Best Hyper parameters for the model are: {'units_emb_1': 816, 'units_dense_1': 380, 'units_dense_2': 78, 'units_dense_3': 122, 'units_drop_1': 0.3, 'units_drop_2': 0.01, 'activation_1': 'tanh', 'activation_2': 'tanh', 'activation_3': 'tanh', 'activation_4': 'tanh', 'learning_rate': 0.0009, 'tuner/epochs': 15, 'tuner/initial_epoch': 5, 'tuner/bracket': 1, 'tuner/round': 1, 'tuner/trial_id': '0023'}\n",
            "The hyperparameter search is complete. The optimal number of units in the first densely-connected layer is 380 and the optimal learning rate for the optimizer is 0.0009.\n"
          ]
        }
      ],
      "source": [
        "tuner.search(train_ds, epochs=50, validation_data=val_ds, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best Hyper parameters for the model are: {best_hps.values}\")\n",
        "print(f\"\"\"The hyperparameter search is complete. The optimal number of units in the first densely-connected layer is {best_hps.get('units_dense_1')} and the optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lak_ylf88xBv"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Find the optimal number of epochs to train the model with the hyperparameters obtained from the search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "McO82AXOuxXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "357b72fa-9038-4f85-f202-43d02baf0023"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "39/39 [==============================] - 2s 50ms/step - loss: 2.3979 - accuracy: 0.2042 - val_loss: 2.5670 - val_accuracy: 0.1200\n",
            "Epoch 2/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 2.1949 - accuracy: 0.2461 - val_loss: 2.3469 - val_accuracy: 0.1200\n",
            "Epoch 3/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 1.7366 - accuracy: 0.4136 - val_loss: 2.3599 - val_accuracy: 0.1600\n",
            "Epoch 4/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 1.3352 - accuracy: 0.6021 - val_loss: 1.8799 - val_accuracy: 0.5600\n",
            "Epoch 5/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 1.0652 - accuracy: 0.7016 - val_loss: 2.1121 - val_accuracy: 0.3200\n",
            "Epoch 6/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.8349 - accuracy: 0.8325 - val_loss: 2.1229 - val_accuracy: 0.3600\n",
            "Epoch 7/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.6371 - accuracy: 0.8953 - val_loss: 1.9343 - val_accuracy: 0.3600\n",
            "Epoch 8/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.5461 - accuracy: 0.9267 - val_loss: 1.8275 - val_accuracy: 0.4000\n",
            "Epoch 9/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.4622 - accuracy: 0.9634 - val_loss: 1.3354 - val_accuracy: 0.6800\n",
            "Epoch 10/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.3760 - accuracy: 0.9791 - val_loss: 1.3178 - val_accuracy: 0.6800\n",
            "Epoch 11/50\n",
            "39/39 [==============================] - 2s 47ms/step - loss: 0.3281 - accuracy: 0.9791 - val_loss: 1.3513 - val_accuracy: 0.6400\n",
            "Epoch 12/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.2662 - accuracy: 0.9895 - val_loss: 1.2724 - val_accuracy: 0.6400\n",
            "Epoch 13/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.2422 - accuracy: 0.9843 - val_loss: 1.3464 - val_accuracy: 0.6800\n",
            "Epoch 14/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.1987 - accuracy: 0.9948 - val_loss: 1.2270 - val_accuracy: 0.6400\n",
            "Epoch 15/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.1806 - accuracy: 0.9948 - val_loss: 1.3270 - val_accuracy: 0.6800\n",
            "Epoch 16/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.1644 - accuracy: 0.9948 - val_loss: 1.2913 - val_accuracy: 0.6400\n",
            "Epoch 17/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.1454 - accuracy: 0.9948 - val_loss: 1.2929 - val_accuracy: 0.7200\n",
            "Epoch 18/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.1268 - accuracy: 0.9948 - val_loss: 1.2432 - val_accuracy: 0.6400\n",
            "Epoch 19/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.1209 - accuracy: 0.9895 - val_loss: 1.2184 - val_accuracy: 0.6800\n",
            "Epoch 20/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.1106 - accuracy: 0.9948 - val_loss: 1.4833 - val_accuracy: 0.6800\n",
            "Epoch 21/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.1063 - accuracy: 0.9948 - val_loss: 1.1845 - val_accuracy: 0.6800\n",
            "Epoch 22/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0977 - accuracy: 0.9948 - val_loss: 1.2249 - val_accuracy: 0.7200\n",
            "Epoch 23/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0931 - accuracy: 0.9895 - val_loss: 1.4532 - val_accuracy: 0.6400\n",
            "Epoch 24/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0875 - accuracy: 0.9948 - val_loss: 1.0736 - val_accuracy: 0.7200\n",
            "Epoch 25/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0812 - accuracy: 0.9948 - val_loss: 1.3691 - val_accuracy: 0.6400\n",
            "Epoch 26/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0743 - accuracy: 0.9948 - val_loss: 1.3898 - val_accuracy: 0.6000\n",
            "Epoch 27/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0688 - accuracy: 1.0000 - val_loss: 1.1863 - val_accuracy: 0.7200\n",
            "Epoch 28/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0705 - accuracy: 0.9895 - val_loss: 1.3002 - val_accuracy: 0.6400\n",
            "Epoch 29/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0684 - accuracy: 0.9948 - val_loss: 1.4366 - val_accuracy: 0.6400\n",
            "Epoch 30/50\n",
            "39/39 [==============================] - 2s 47ms/step - loss: 0.0614 - accuracy: 0.9895 - val_loss: 1.4276 - val_accuracy: 0.6800\n",
            "Epoch 31/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0586 - accuracy: 0.9948 - val_loss: 1.3006 - val_accuracy: 0.6800\n",
            "Epoch 32/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0550 - accuracy: 0.9948 - val_loss: 1.2890 - val_accuracy: 0.6400\n",
            "Epoch 33/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0525 - accuracy: 0.9948 - val_loss: 1.3779 - val_accuracy: 0.6800\n",
            "Epoch 34/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0532 - accuracy: 0.9948 - val_loss: 1.3174 - val_accuracy: 0.6800\n",
            "Epoch 35/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0497 - accuracy: 0.9895 - val_loss: 1.2656 - val_accuracy: 0.6800\n",
            "Epoch 36/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0468 - accuracy: 0.9948 - val_loss: 1.3020 - val_accuracy: 0.6800\n",
            "Epoch 37/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0456 - accuracy: 0.9948 - val_loss: 1.3652 - val_accuracy: 0.6800\n",
            "Epoch 38/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0436 - accuracy: 0.9948 - val_loss: 1.3630 - val_accuracy: 0.6800\n",
            "Epoch 39/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0419 - accuracy: 0.9948 - val_loss: 1.3831 - val_accuracy: 0.6800\n",
            "Epoch 40/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0407 - accuracy: 0.9948 - val_loss: 1.3823 - val_accuracy: 0.6800\n",
            "Epoch 41/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0392 - accuracy: 0.9948 - val_loss: 1.3522 - val_accuracy: 0.6800\n",
            "Epoch 42/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0379 - accuracy: 0.9948 - val_loss: 1.4539 - val_accuracy: 0.6800\n",
            "Epoch 43/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0365 - accuracy: 0.9948 - val_loss: 1.3553 - val_accuracy: 0.6800\n",
            "Epoch 44/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0354 - accuracy: 0.9948 - val_loss: 1.3675 - val_accuracy: 0.6800\n",
            "Epoch 45/50\n",
            "39/39 [==============================] - 2s 47ms/step - loss: 0.0341 - accuracy: 0.9948 - val_loss: 1.3550 - val_accuracy: 0.6800\n",
            "Epoch 46/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0332 - accuracy: 0.9948 - val_loss: 1.4175 - val_accuracy: 0.6400\n",
            "Epoch 47/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0322 - accuracy: 0.9948 - val_loss: 1.4054 - val_accuracy: 0.6800\n",
            "Epoch 48/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0313 - accuracy: 0.9948 - val_loss: 1.4113 - val_accuracy: 0.6400\n",
            "Epoch 49/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0304 - accuracy: 0.9948 - val_loss: 1.4343 - val_accuracy: 0.6400\n",
            "Epoch 50/50\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.0296 - accuracy: 0.9948 - val_loss: 1.4437 - val_accuracy: 0.6800\n",
            "Best epoch: 17\n"
          ]
        }
      ],
      "source": [
        "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(train_ds, epochs=50, validation_data=val_ds)\n",
        "\n",
        "val_acc_per_epoch = history.history['val_accuracy']\n",
        "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOTSirSTI3Gp"
      },
      "source": [
        "Re-instantiate the hypermodel and train it with the optimal number of epochs from above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NoiPUEHmMhCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b37423-5700-4f64-a9ad-d254aaddee29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/17\n",
            "39/39 [==============================] - 3s 50ms/step - loss: 2.4035 - accuracy: 0.2251 - val_loss: 2.4736 - val_accuracy: 0.1200\n",
            "Epoch 2/17\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 2.1351 - accuracy: 0.2565 - val_loss: 2.1970 - val_accuracy: 0.2000\n",
            "Epoch 3/17\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 1.6527 - accuracy: 0.4188 - val_loss: 2.2210 - val_accuracy: 0.1600\n",
            "Epoch 4/17\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 1.2549 - accuracy: 0.6021 - val_loss: 1.8638 - val_accuracy: 0.3600\n",
            "Epoch 5/17\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.8767 - accuracy: 0.7644 - val_loss: 1.6282 - val_accuracy: 0.5200\n",
            "Epoch 6/17\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.6623 - accuracy: 0.8743 - val_loss: 1.7709 - val_accuracy: 0.3200\n",
            "Epoch 7/17\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.5375 - accuracy: 0.8901 - val_loss: 1.5485 - val_accuracy: 0.5200\n",
            "Epoch 8/17\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.4629 - accuracy: 0.9267 - val_loss: 1.8196 - val_accuracy: 0.4000\n",
            "Epoch 9/17\n",
            "39/39 [==============================] - 2s 47ms/step - loss: 0.4202 - accuracy: 0.9424 - val_loss: 2.4861 - val_accuracy: 0.2000\n",
            "Epoch 10/17\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.3721 - accuracy: 0.9529 - val_loss: 2.0301 - val_accuracy: 0.3600\n",
            "Epoch 11/17\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.4230 - accuracy: 0.9529 - val_loss: 1.6538 - val_accuracy: 0.5200\n",
            "Epoch 12/17\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.3493 - accuracy: 0.9581 - val_loss: 1.4167 - val_accuracy: 0.6000\n",
            "Epoch 13/17\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.2966 - accuracy: 0.9843 - val_loss: 1.4076 - val_accuracy: 0.6000\n",
            "Epoch 14/17\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.2279 - accuracy: 0.9895 - val_loss: 1.4109 - val_accuracy: 0.6000\n",
            "Epoch 15/17\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.1916 - accuracy: 0.9948 - val_loss: 1.3671 - val_accuracy: 0.7200\n",
            "Epoch 16/17\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.1668 - accuracy: 0.9948 - val_loss: 1.3875 - val_accuracy: 0.6000\n",
            "Epoch 17/17\n",
            "39/39 [==============================] - 2s 46ms/step - loss: 0.1501 - accuracy: 0.9948 - val_loss: 1.2534 - val_accuracy: 0.6400\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f382646b310>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Retrain the model\n",
        "hypermodel.fit(train_ds, epochs=best_epoch, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation and Metrics"
      ],
      "metadata": {
        "id": "IQFkUEWLtLm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation\n",
        "\n",
        "eval_result = hypermodel.evaluate(test_ds)\n",
        "print(\"[test loss, test accuracy]:\", eval_result)\n",
        "\n",
        "# Model Metrics\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# y_labels = val_ds.map(lambda x,y: y)\n",
        "\n",
        "# y_labels = y_labels.as_numpy_iterator()\n",
        "\n",
        "## Test_ds metrics\n",
        "print(\"Test dataset metrics:\")\n",
        "# Reduce to 1D Array\n",
        "y_labels = np.concatenate([y for x, y in test_ds], axis=0)\n",
        "\n",
        "y_hat_probs = hypermodel.predict(test_ds)\n",
        "\n",
        "y_hat_classes = y_hat_probs.argmax(axis=-1)\n",
        "\n",
        "\n",
        "## accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(y_labels, y_hat_classes)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "\n",
        "## precision tp / (tp + fp)\n",
        "precision = precision_score(y_labels, y_hat_classes, average=None)\n",
        "print('Precision (None): %f' % precision.sum())\n",
        "precision = precision_score(y_labels, y_hat_classes, average='micro')\n",
        "print('Precision (micro): %f' % precision)\n",
        "precision = precision_score(y_labels, y_hat_classes, average= 'macro')\n",
        "print('Precision (macro): %f' % precision)\n",
        "# precision = precision_score(y_labels, y_hat_classes, average= 'samples')\n",
        "# print('Precision (samples): %f' % precision)\n",
        "precision = precision_score(y_labels, y_hat_classes, average= 'weighted')\n",
        "print('Precision (weighted): %f' % precision)\n",
        "\n",
        "## recall: tp / (tp + fn)\n",
        "recall = recall_score(y_labels, y_hat_classes, average='micro')\n",
        "print('Recall (micro): %f' % recall)\n",
        "recall = recall_score(y_labels, y_hat_classes, average='macro')\n",
        "print('Recall (macro): %f' % recall)\n",
        "# recall = recall_score(y_labels, y_hat_classes, average='samples')\n",
        "# print('Recall (samples): %f' % recall)\n",
        "recall = recall_score(y_labels, y_hat_classes, average='weighted')\n",
        "print('Recall (weighted): %f' % recall)\n",
        "\n",
        "## f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_labels, y_hat_classes, average='micro')\n",
        "print('F1 score (micro): %f' % f1)\n",
        "f1 = f1_score(y_labels, y_hat_classes, average='macro')\n",
        "print('F1 score (macro): %f' % f1)\n",
        "f1 = f1_score(y_labels, y_hat_classes, average='weighted')\n",
        "print('F1 score (weighted): %f' % f1)\n",
        " \n",
        "## kappa\n",
        "kappa = cohen_kappa_score(y_labels, y_hat_classes)\n",
        "print('Cohens kappa: %f' % kappa)\n",
        "\n",
        "## ROC AUC\n",
        "# print(np.sum(y_hat_probs))\n",
        "# y_hat_probs = y_hat_probs / np.sum(y_hat_probs)\n",
        "# print(np.sum(y_hat_probs))\n",
        "# auc = roc_auc_score(y_labels, y_hat_probs, multi_class='ovo')\n",
        "# print('ROC AUC: %f' % auc)\n",
        "\n",
        "## confusion matrix\n",
        "matrix = confusion_matrix(y_labels, y_hat_classes)\n",
        "print(matrix)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "## Val_ds metrics \n",
        "print(\"Validation dataset metrics:\")\n",
        "\n",
        "# Reduce to 1D Array\n",
        "y_labels = np.concatenate([y for x, y in val_ds], axis=0)\n",
        "\n",
        "y_hat_probs = hypermodel.predict(val_ds)\n",
        "\n",
        "y_hat_classes = y_hat_probs.argmax(axis=-1)\n",
        "\n",
        "\n",
        "## accuracy: (tp + tn) / (p + n)\n",
        "accuracy = accuracy_score(y_labels, y_hat_classes)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "\n",
        "## precision tp / (tp + fp)\n",
        "precision = precision_score(y_labels, y_hat_classes, average=None)\n",
        "print('Precision (None): %f' % precision.sum())\n",
        "precision = precision_score(y_labels, y_hat_classes, average='micro')\n",
        "print('Precision (micro): %f' % precision)\n",
        "precision = precision_score(y_labels, y_hat_classes, average= 'macro')\n",
        "print('Precision (macro): %f' % precision)\n",
        "# precision = precision_score(y_labels, y_hat_classes, average= 'samples')\n",
        "# print('Precision (samples): %f' % precision)\n",
        "precision = precision_score(y_labels, y_hat_classes, average= 'weighted')\n",
        "print('Precision (weighted): %f' % precision)\n",
        "\n",
        "## recall: tp / (tp + fn)\n",
        "recall = recall_score(y_labels, y_hat_classes, average='micro')\n",
        "print('Recall (micro): %f' % recall)\n",
        "recall = recall_score(y_labels, y_hat_classes, average='macro')\n",
        "print('Recall (macro): %f' % recall)\n",
        "# recall = recall_score(y_labels, y_hat_classes, average='samples')\n",
        "# print('Recall (samples): %f' % recall)\n",
        "recall = recall_score(y_labels, y_hat_classes, average='weighted')\n",
        "print('Recall (weighted): %f' % recall)\n",
        "\n",
        "## f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(y_labels, y_hat_classes, average='micro')\n",
        "print('F1 score (micro): %f' % f1)\n",
        "f1 = f1_score(y_labels, y_hat_classes, average='macro')\n",
        "print('F1 score (macro): %f' % f1)\n",
        "f1 = f1_score(y_labels, y_hat_classes, average='weighted')\n",
        "print('F1 score (weighted): %f' % f1)\n",
        " \n",
        "## kappa\n",
        "kappa = cohen_kappa_score(y_labels, y_hat_classes)\n",
        "print('Cohens kappa: %f' % kappa)\n",
        "\n",
        "## ROC AUC\n",
        "# print(np.sum(y_hat_probs))\n",
        "# y_hat_probs = y_hat_probs / np.sum(y_hat_probs)\n",
        "# print(np.sum(y_hat_probs))\n",
        "# auc = roc_auc_score(y_labels, y_hat_probs, multi_class='ovo')\n",
        "# print('ROC AUC: %f' % auc)\n",
        "\n",
        "## confusion matrix\n",
        "matrix = confusion_matrix(y_labels, y_hat_classes)\n",
        "print(matrix)"
      ],
      "metadata": {
        "id": "GDZ3ySZQsasX",
        "outputId": "be16a6c1-95f9-4742-9f44-4b1fb08a62d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 5ms/step - loss: 0.9066 - accuracy: 0.7727\n",
            "[test loss, test accuracy]: [0.9065622687339783, 0.7727272510528564]\n",
            "Test dataset metrics:\n",
            "Accuracy: 0.772727\n",
            "Precision (None): 7.333333\n",
            "Precision (micro): 0.772727\n",
            "Precision (macro): 0.733333\n",
            "Precision (weighted): 0.712121\n",
            "Recall (micro): 0.772727\n",
            "Recall (macro): 0.816667\n",
            "Recall (weighted): 0.772727\n",
            "F1 score (micro): 0.772727\n",
            "F1 score (macro): 0.746667\n",
            "F1 score (weighted): 0.721212\n",
            "Cohens kappa: 0.736211\n",
            "[[1 0 0 0 0 0 0 0 0 0]\n",
            " [0 5 0 0 0 0 0 0 0 0]\n",
            " [1 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 2 0 1 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 4 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 1 2 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 1]]\n",
            "\n",
            "\n",
            "\n",
            "Validation dataset metrics:\n",
            "Accuracy: 0.640000\n",
            "Precision (None): 6.933333\n",
            "Precision (micro): 0.640000\n",
            "Precision (macro): 0.630303\n",
            "Precision (weighted): 0.665333\n",
            "Recall (micro): 0.640000\n",
            "Recall (macro): 0.733333\n",
            "Recall (weighted): 0.640000\n",
            "F1 score (micro): 0.640000\n",
            "F1 score (macro): 0.626623\n",
            "F1 score (weighted): 0.599524\n",
            "Cohens kappa: 0.601770\n",
            "[[1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 2 1 0 0 0 0 0 0 0 0]\n",
            " [0 1 1 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 1 0 1]\n",
            " [0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 3 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 3 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 1 0 0 0 0 1 0 1 2 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQRpPHZsz-eC"
      },
      "source": [
        "The `my_dir/intro_to_kt` directory contains detailed logs and checkpoints for every trial (model configuration) run during the hyperparameter search. If you re-run the hyperparameter search, the Keras Tuner uses the existing state from these logs to resume the search. To disable this behavior, pass an additional `overwrite=True` argument while instantiating the tuner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKwLOzKpFGAj"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial, you learned how to use the Keras Tuner to tune hyperparameters for a model. To learn more about the Keras Tuner, check out these additional resources:\n",
        "\n",
        "* [Keras Tuner on the TensorFlow blog](https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html)\n",
        "* [Keras Tuner website](https://keras-team.github.io/keras-tuner/)\n",
        "\n",
        "Also check out the [HParams Dashboard](https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams) in TensorBoard to interactively tune your model hyperparameters."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "keras_tuner.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}