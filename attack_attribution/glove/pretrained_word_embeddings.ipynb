{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A0HHUcSKmSk"
      },
      "source": [
        "# Using pre-trained word embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6Y-1E-QKmSm"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "YyPAQ-qlKmSm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qml8NyDUKmSn"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this example, we show how to train a text classification model that uses pre-trained\n",
        "word embeddings.\n",
        "\n",
        "We'll work with threat intelligence report dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eyHuc6bKmSo"
      },
      "source": [
        "## Download the Threat Intelligence data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU98N1hXKmSo",
        "outputId": "77c820a0-d188-4497-e327-aa3c55d108bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./AttackAttributionDataset-master\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['APT29',\n",
              " 'Lazarus',\n",
              " 'FIN7',\n",
              " 'APT17',\n",
              " 'Winnti',\n",
              " 'DeepPanda',\n",
              " 'RocketKitten',\n",
              " 'APT28',\n",
              " 'Turla',\n",
              " 'menuPass',\n",
              " 'README.md',\n",
              " 'OilRig',\n",
              " 'APT3']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "url = 'https://github.com/eyalmazuz/AttackAttributionDataset/archive/refs/heads/master.zip' \n",
        "\n",
        "dataset = tf.keras.utils.get_file('master.zip', url,\n",
        "                                  extract=True, cache_dir='.',\n",
        "                                  cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'AttackAttributionDataset-master')\n",
        "print(dataset_dir)\n",
        "os.listdir(dataset_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa7IkDFwKmSr"
      },
      "source": [
        "## Shuffle and split the data into training & validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQzA3GMJLMOX",
        "outputId": "aa4f0ba5-de86-4da1-92f4-607c6cd0d19e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 238 files belonging to 12 classes.\n",
            "Using 191 files for training.\n",
            "Found 238 files belonging to 12 classes.\n",
            "Using 47 files for validation.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 5\n",
        "seed = 42\n",
        "\n",
        "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'AttackAttributionDataset-master',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)\n",
        "\n",
        "\n",
        "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'AttackAttributionDataset-master', batch_size=batch_size, validation_split=0.2,\n",
        "    subset='validation', seed=seed)\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB-vY4LoKmSs"
      },
      "source": [
        "## Create a vocabulary index\n",
        "\n",
        "Let's use the `TextVectorization` to index the vocabulary found in the dataset.\n",
        "Later, we'll use the same layer instance to vectorize the samples.\n",
        "\n",
        "Our layer will only consider the top 20,000 words, and will truncate or pad sequences to\n",
        "be actually 200 tokens long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FZChL2KZKmSs"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "vectorize_layer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
        "text_ds = train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xt7d3doKmSt"
      },
      "source": [
        "You can retrieve the computed vocabulary used via `vectorizer.get_vocabulary()`. Let's\n",
        "print the top 5 words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiTfxHA2KmSt",
        "outputId": "bfda743a-2120-4010-970c-e87890b3a94e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'to', 'of']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorize_layer.get_vocabulary()[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kYXlsMBKmSt"
      },
      "source": [
        "Let's vectorize a test sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axQja8WYKmSt",
        "outputId": "b9bed445-b4ea-4877-a1ab-62a01960e6a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([    2, 17174, 12257,    14,     2,     1])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = vectorize_layer([[\"the cat sat on the mat\"]])\n",
        "output.numpy()[0, :6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsXSzQyUKmSu"
      },
      "source": [
        "As you can see, \"the\" gets represented as \"2\". Why not 0, given that \"the\" was the first\n",
        "word in the vocabulary? That's because index 0 is reserved for padding and index 1 is\n",
        "reserved for \"out of vocabulary\" tokens.\n",
        "\n",
        "Here's a dict mapping words to their indices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qCP7ayINKmSu"
      },
      "outputs": [],
      "source": [
        "voc = vectorize_layer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovUW0y7KKmSv"
      },
      "source": [
        "As you can see, we obtain the same encoding as above for our test sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9DInAAuYKmSv"
      },
      "outputs": [],
      "source": [
        "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "# [word_index[w] for w in test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq_VRVPkKmSv"
      },
      "source": [
        "## Load pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx0TyWvlKmSv"
      },
      "source": [
        "Let's download pre-trained GloVe embeddings (a 822M zip file)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0oUVB_3MjgE",
        "outputId": "e3ae1d4f-17ed-4c70-c84f-f8083e8bba06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-08-19 22:21:23--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-08-19 22:21:24--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-08-19 22:21:24--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.04MB/s    in 2m 39s  \n",
            "\n",
            "2022-08-19 22:24:03 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88kTsqKmKmSv"
      },
      "source": [
        "The archive contains text-encoded vectors of various sizes: 50-dimensional,\n",
        "100-dimensional, 200-dimensional, 300-dimensional. We'll use the 100D ones.\n",
        "\n",
        "Let's make a dict mapping words (strings) to their NumPy vector representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waBGT_OUKmSv",
        "outputId": "af32ca57-dd26-4c6a-bd2f-2786cf871ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "path_to_glove_file = os.path.join(\n",
        "    os.path.dirname(dataset), \"glove.6B.100d.txt\"\n",
        ")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW3qJenYKmSw"
      },
      "source": [
        "Now, let's prepare a corresponding embedding matrix that we can use in a Keras\n",
        "`Embedding` layer. It's a simple NumPy matrix where entry at index `i` is the pre-trained\n",
        "vector for the word of index `i` in our `vectorizer`'s vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzEPnUPuKmSw",
        "outputId": "64e3511f-da6c-4350-83d7-5ceb20866c0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted 12056 words (6898 misses)\n"
          ]
        }
      ],
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lx4zKu5KmSw"
      },
      "source": [
        "Next, we load the pre-trained word embeddings matrix into an `Embedding` layer.\n",
        "\n",
        "Note that we set `trainable=False` so as to keep the embeddings fixed (we don't want to\n",
        "update them during training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Zf8_HfUgKmSw"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMZ8L_shKmSw"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "A simple 1D convnet with global max pooling and a classifier at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA9Go98ZKmSw",
        "outputId": "b7de33a9-01a3-4dd8-b4b0-9b4264157100"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_1 (TextV  (None, 200)              0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding (Embedding)       multiple                  1895600   \n",
            "                                                                 \n",
            " global_average_pooling1d_9   (None, 100)              0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 384)               38784     \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 384)               0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 192)               73920     \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 192)               0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 16)                3088      \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 12)                204       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,011,596\n",
            "Trainable params: 115,996\n",
            "Non-trainable params: 1,895,600\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "model = Sequential([\n",
        "  vectorize_layer,\n",
        "  embedding_layer,\n",
        "  GlobalAveragePooling1D(),\n",
        "  Dense(384, activation=\"relu\"),\n",
        "  tf.keras.layers.Dropout(0.1),  Dense(192, activation=\"relu\"),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  Dense(16, activation='relu'),\n",
        "  Dense(len(class_names))\n",
        "])\n",
        "\n",
        "# int_sequences_input = keras.Input(shape=(None,), dtype=tf.string)\n",
        "# embedded_sequences = embedding_layer(int_sequences_input)\n",
        "# x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
        "# x = layers.MaxPooling1D(5)(x)\n",
        "# x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "# x = layers.MaxPooling1D(5)(x)\n",
        "# x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "# x = layers.Flatten()(embedded_sequences)\n",
        "# x = layers.GlobalAveragePooling1D()(x)\n",
        "# x = layers.Dense(128, activation=\"relu\")(x)\n",
        "# x = layers.Dropout(0.5)(x)\n",
        "# preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
        "# model = keras.Model(int_sequences_input, preds)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXIfpxN_KmSx"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "First, convert our list-of-strings data to NumPy arrays of integer indices. The arrays\n",
        "are right-padded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "dYHQLcKkKmSx"
      },
      "outputs": [],
      "source": [
        "# x_train = vectorize_layer(np.array([[s] for s in train_ds])).numpy()\n",
        "# x_val = vectorize_layer(np.array([[s] for s in val_ds])).numpy()\n",
        "\n",
        "# y_train = np.array(train_labels)\n",
        "# y_val = np.array(val_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvVJRjBTKmSx"
      },
      "source": [
        "We use categorical crossentropy as our loss since we're doing softmax classification.\n",
        "Moreover, we use `sparse_categorical_crossentropy` since our labels are integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2_JSV4qKmSx",
        "outputId": "f393a5e7-7d83-41bd-8725-84d472c543d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "39/39 [==============================] - 4s 12ms/step - loss: 6.0079 - acc: 0.1414 - val_loss: 2.4849 - val_acc: 0.1915\n",
            "Epoch 2/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 3.4985 - acc: 0.1361 - val_loss: 2.4849 - val_acc: 0.1489\n",
            "Epoch 3/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 2.5284 - acc: 0.1780 - val_loss: 2.4849 - val_acc: 0.1915\n",
            "Epoch 4/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 2.5935 - acc: 0.1257 - val_loss: 2.4849 - val_acc: 0.1489\n",
            "Epoch 5/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 2.5175 - acc: 0.0785 - val_loss: 2.4849 - val_acc: 0.1489\n",
            "Epoch 6/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 2.4849 - acc: 0.0785 - val_loss: 2.4849 - val_acc: 0.1489\n",
            "Epoch 7/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 2.6386 - acc: 0.0733 - val_loss: 2.4849 - val_acc: 0.1489\n",
            "Epoch 8/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 2.5359 - acc: 0.0785 - val_loss: 2.4849 - val_acc: 0.1489\n",
            "Epoch 9/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 2.4795 - acc: 0.2304 - val_loss: 2.4849 - val_acc: 0.1915\n",
            "Epoch 10/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 2.4849 - acc: 0.2408 - val_loss: 2.4849 - val_acc: 0.1915\n",
            "Epoch 11/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 2.4849 - acc: 0.2408 - val_loss: 2.4849 - val_acc: 0.1915\n",
            "Epoch 12/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 2.4849 - acc: 0.2408 - val_loss: 2.4849 - val_acc: 0.1915\n",
            "Epoch 13/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 2.4849 - acc: 0.2408 - val_loss: 2.4849 - val_acc: 0.1915\n",
            "Epoch 14/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 2.5260 - acc: 0.1623 - val_loss: 2.4849 - val_acc: 0.1489\n",
            "Epoch 15/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 2.5306 - acc: 0.0838 - val_loss: 2.4849 - val_acc: 0.1489\n",
            "Epoch 16/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 2.5514 - acc: 0.0785 - val_loss: 2.4849 - val_acc: 0.1489\n",
            "Epoch 17/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 2.4849 - acc: 0.0785 - val_loss: 2.4849 - val_acc: 0.1489\n",
            "Epoch 18/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 2.4849 - acc: 0.0785 - val_loss: 2.4849 - val_acc: 0.1489\n",
            "Epoch 19/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 2.4849 - acc: 0.0838 - val_loss: 2.4849 - val_acc: 0.1489\n",
            "Epoch 20/20\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 2.4849 - acc: 0.0785 - val_loss: 2.4849 - val_acc: 0.1489\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdf8adc02d0>"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
        ")\n",
        "model.fit(train_ds, epochs=20, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVLJit48KmSy"
      },
      "source": [
        "## Export an end-to-end model\n",
        "\n",
        "Now, we may want to export a `Model` object that takes as input a string of arbitrary\n",
        "length, rather than a sequence of indices. It would make the model much more portable,\n",
        "since you wouldn't have to worry about the input preprocessing pipeline.\n",
        "\n",
        "Our `vectorizer` is actually a Keras layer, so it's simple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "DZhcdQd8KmSy",
        "outputId": "a83b204c-4534-4f2e-c6ac-1e390924fb90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Turla'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = vectorize_layer(string_input)\n",
        "preds = model.predict(val_ds)\n",
        "# end_to_end_model = keras.Model(string_input, preds)\n",
        "\n",
        "# probabilities = end_to_end_model.predict(\n",
        "#     [[\"this message is about computer graphics and 3D modeling\"]]\n",
        "# )\n",
        "\n",
        "classes = np.argmax(preds)\n",
        "print(classes)\n",
        "class_names[np.argmax(preds)]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "pretrained_word_embeddings",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
