{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yo62ffS5TF5"
      },
      "source": [
        "# Using text and neural network features\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/decision_forests/tutorials/intermediate_colab\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/decision-forests/blob/main/documentation/tutorials/intermediate_colab.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/decision-forests/blob/main/documentation/tutorials/intermediate_colab.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/decision-forests/documentation/tutorials/intermediate_colab.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://tfhub.dev/google/universal-sentence-encoder/4\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub model</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrCwCCxhiAL7"
      },
      "source": [
        "Welcome to the **Intermediate Colab** for **TensorFlow Decision Forests** (**TF-DF**).\n",
        "In this colab, you will learn about some more advanced capabilities of **TF-DF**, including how to deal with natural language features.\n",
        "\n",
        "This colab assumes you are familiar with the concepts presented the [Beginner colab](beginner_colab.ipynb), notably about the installation about TF-DF.\n",
        "\n",
        "In this colab, you will:\n",
        "\n",
        "1. Train a Random Forest that consumes text features natively as categorical sets.\n",
        "\n",
        "1. Train a Random Forest that consumes text features using a [TensorFlow Hub](https://www.tensorflow.org/hub) module. In this setting (transfer learning), the module is already pre-trained on a large text corpus.\n",
        "\n",
        "1. Train a Gradient Boosted Decision Trees (GBDT) and a Neural Network together. The GBDT will consume the output of the Neural Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzskapxq7gdo"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZiInVYfffAb",
        "outputId": "c7df883e-eba3-47dc-966a-321acfa52ab9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_decision_forests\n",
            "  Downloading tensorflow_decision_forests-0.2.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.0 MB 10.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (1.3.5)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (0.37.1)\n",
            "Collecting wurlitzer\n",
            "  Downloading wurlitzer-3.0.2-py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (1.15.0)\n",
            "Collecting tensorflow~=2.9.1\n",
            "  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 4.7 kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (1.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (1.1.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (1.47.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (14.0.6)\n",
            "Collecting flatbuffers<2,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (21.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (1.14.1)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 68.0 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 76.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (4.1.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (3.17.3)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 68.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (0.26.0)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.9.1->tensorflow_decision_forests) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (2022.6.15)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow~=2.9.1->tensorflow_decision_forests) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tensorflow_decision_forests) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tensorflow_decision_forests) (2.8.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, flatbuffers, wurlitzer, tensorflow, tensorflow-decision-forests\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "Successfully installed flatbuffers-1.12 gast-0.4.0 keras-2.9.0 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-decision-forests-0.2.7 tensorflow-estimator-2.9.0 wurlitzer-3.0.2\n"
          ]
        }
      ],
      "source": [
        "# Install TensorFlow Dececision Forests\n",
        "!pip install tensorflow_decision_forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EFndCFdoJM5"
      },
      "source": [
        "[Wurlitzer](https://pypi.org/project/wurlitzer/) is needed to display the detailed training logs in Colabs (when using `verbose=2` in the model constructor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L06XWRdSoLj5",
        "outputId": "9f559415-6477-4795-ac37-a4e750651dd9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.7/dist-packages (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wurlitzer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7PlfbnxYcPf"
      },
      "source": [
        "Import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "RsCV2oAS7gC_",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import tensorflow_decision_forests as tfdf\n",
        "\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import math\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2fsI0y5x5i5"
      },
      "source": [
        "The hidden code cell limits the output height in colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jZXB4o6Tlu0i",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "from IPython.core.magic import register_line_magic\n",
        "from IPython.display import Javascript\n",
        "from IPython.display import display as ipy_display\n",
        "\n",
        "# Some of the model training logs can cover the full\n",
        "# screen if not compressed to a smaller viewport.\n",
        "# This magic allows setting a max height for a cell.\n",
        "@register_line_magic\n",
        "def set_cell_height(size):\n",
        "  ipy_display(\n",
        "      Javascript(\"google.colab.output.setIframeHeight(0, true, {maxHeight: \" +\n",
        "                 str(size) + \"})\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_D4Ft4o65XT"
      },
      "source": [
        "## Use raw text as features\n",
        "\n",
        "TF-DF can consume [categorical-set](https://arxiv.org/pdf/2009.09991.pdf) features natively. Categorical-sets represent text features as bags of words (or n-grams).\n",
        "\n",
        "For example: `\"The little blue dog\" ` → `{\"the\", \"little\", \"blue\", \"dog\"}`\n",
        "\n",
        "In this example, you'll will train a Random Forest on the [Threat Intelligence Reports](https://github.com/threatDataset) dataset. The objective of this dataset is to classify reports into 12 threat actors. You'll will use the multiclass classification version of the dataset.\n",
        "\n",
        "**Note:** Categorical-set features can be expensive to train. In this colab, we will train a small Random Forest with 1000 trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVN-j0E4Q1T3",
        "outputId": "a4028aa0-b166-4764-e983-62a3ff2c7683",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/eyalmazuz/AttackAttributionDataset/archive/refs/heads/master.zip\n",
            "1248841/1248841 [==============================] - 0s 0us/step\n",
            "Found 238 files belonging to 12 classes.\n",
            "Using 191 files for training.\n",
            "Found 238 files belonging to 12 classes.\n",
            "Using 47 files for validation.\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "\n",
        "url = 'https://github.com/eyalmazuz/AttackAttributionDataset/archive/refs/heads/master.zip' \n",
        "\n",
        "dataset = tf.keras.utils.get_file('master.zip', url, extract=True, cache_dir='.', cache_subdir='')\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 5\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'AttackAttributionDataset-master',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    shuffle=False,\n",
        "    seed=seed)\n",
        "\n",
        "class_names = raw_train_ds.class_names\n",
        "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'AttackAttributionDataset-master',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    shuffle=False,\n",
        "    subset='validation',\n",
        "    seed=seed)\n",
        "\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "# Display the first 3 examples of the test fold.\n",
        "# for example in train_ds.take(1):\n",
        "#   print({attr_name: attr_tensor.numpy() for attr_name, attr_tensor in example.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYkIjROI9w43"
      },
      "source": [
        "Finaly, train and evaluate the model as usual. TF-DF  automatically detects multi-valued categorical features as categorical-set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "mpxTtYo39wYZ",
        "outputId": "330fd460-147f-4c84-e3cd-c9b24da0a451",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Use 4 thread(s) for training\n",
            "Use /tmp/tmpm782msh0 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training tensor examples:\n",
            "Features: Tensor(\"data:0\", shape=(None,), dtype=string)\n",
            "Label: Tensor(\"data_1:0\", shape=(None,), dtype=int32)\n",
            "Weights: None\n",
            "Normalized tensor features:\n",
            " {'data:0': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data:0' shape=(None,) dtype=string>)}\n",
            "Training dataset read in 0:00:04.005984. Found 191 examples.\n",
            "Training model...\n",
            "Standard output detected as not visible to the user e.g. running in a notebook. Creating a training log redirection. If training get stuck, try calling tfdf.keras.set_training_logs_redirection(False).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[INFO kernel.cc:813] Start Yggdrasil model training\n",
            "[INFO kernel.cc:814] Collect training examples\n",
            "[INFO kernel.cc:422] Number of batches: 39\n",
            "[INFO kernel.cc:423] Number of examples: 191\n",
            "[INFO data_spec_inference.cc:303] 190 item(s) have been pruned (i.e. they are considered out of dictionary) for the column data:0 (0 item(s) left) because min_value_count=5 and max_number_of_unique_values=2000\n",
            "[INFO kernel.cc:836] Training dataset:\n",
            "Number of records: 191\n",
            "Number of columns: 2\n",
            "\n",
            "Number of columns by type:\n",
            "\tCATEGORICAL: 2 (100%)\n",
            "\n",
            "Columns:\n",
            "\n",
            "CATEGORICAL: 2 (100%)\n",
            "\t0: \"data:0\" CATEGORICAL has-dict vocab-size:1 num-oods:190 (99.4764%)\n",
            "\t1: \"__LABEL\" CATEGORICAL integerized vocab-size:10 no-ood-item\n",
            "\n",
            "Terminology:\n",
            "\tnas: Number of non-available (i.e. missing) values.\n",
            "\tood: Out of dictionary.\n",
            "\tmanually-defined: Attribute which type is manually defined by the user i.e. the type was not automatically inferred.\n",
            "\ttokenized: The attribute value is obtained through tokenization.\n",
            "\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n",
            "\tvocab-size: Number of unique values.\n",
            "\n",
            "[INFO kernel.cc:882] Configure learner\n",
            "[INFO kernel.cc:912] Training config:\n",
            "learner: \"RANDOM_FOREST\"\n",
            "features: \"data:0\"\n",
            "label: \"__LABEL\"\n",
            "task: CLASSIFICATION\n",
            "random_seed: 123456\n",
            "metadata {\n",
            "  framework: \"TF Keras\"\n",
            "}\n",
            "pure_serving_model: false\n",
            "[yggdrasil_decision_forests.model.random_forest.proto.random_forest_config] {\n",
            "  num_trees: 1000\n",
            "  decision_tree {\n",
            "    max_depth: 16\n",
            "    min_examples: 5\n",
            "    in_split_min_examples_check: true\n",
            "    keep_non_leaf_label_distribution: true\n",
            "    num_candidate_attributes: 0\n",
            "    missing_value_policy: GLOBAL_IMPUTATION\n",
            "    allow_na_conditions: false\n",
            "    categorical_set_greedy_forward {\n",
            "      sampling: 0.1\n",
            "      max_num_items: -1\n",
            "      min_item_frequency: 1\n",
            "    }\n",
            "    growing_strategy_local {\n",
            "    }\n",
            "    categorical {\n",
            "      cart {\n",
            "      }\n",
            "    }\n",
            "    axis_aligned_split {\n",
            "    }\n",
            "    internal {\n",
            "      sorting_strategy: PRESORTED\n",
            "    }\n",
            "    uplift {\n",
            "      min_examples_in_treatment: 5\n",
            "      split_score: KULLBACK_LEIBLER\n",
            "    }\n",
            "  }\n",
            "  winner_take_all_inference: true\n",
            "  compute_oob_performances: true\n",
            "  compute_oob_variable_importances: false\n",
            "  num_oob_variable_importances_permutations: 1\n",
            "  bootstrap_training_dataset: true\n",
            "  bootstrap_size_ratio: 1\n",
            "  adapt_bootstrap_size_ratio_for_maximum_training_duration: false\n",
            "  sampling_with_replacement: true\n",
            "}\n",
            "\n",
            "[INFO kernel.cc:915] Deployment config:\n",
            "cache_path: \"/tmp/tmpm782msh0/working_cache\"\n",
            "num_threads: 4\n",
            "try_resume_training: true\n",
            "\n",
            "[INFO kernel.cc:944] Train model\n",
            "[INFO random_forest.cc:407] Training random forest on 191 example(s) and 1 feature(s).\n",
            "[INFO random_forest.cc:796] Training of tree  1/1000 (tree index:0) done accuracy:0.209677 logloss:28.4861\n",
            "[INFO random_forest.cc:796] Training of tree  11/1000 (tree index:10) done accuracy:0.284211 logloss:25.7997\n",
            "[INFO random_forest.cc:796] Training of tree  21/1000 (tree index:20) done accuracy:0.287958 logloss:23.9055\n",
            "[INFO random_forest.cc:796] Training of tree  31/1000 (tree index:31) done accuracy:0.287958 logloss:23.9165\n",
            "[INFO random_forest.cc:796] Training of tree  41/1000 (tree index:41) done accuracy:0.287958 logloss:23.9278\n",
            "[INFO random_forest.cc:796] Training of tree  51/1000 (tree index:50) done accuracy:0.287958 logloss:23.9358\n",
            "[INFO random_forest.cc:796] Training of tree  61/1000 (tree index:60) done accuracy:0.287958 logloss:23.9439\n",
            "[INFO random_forest.cc:796] Training of tree  71/1000 (tree index:70) done accuracy:0.287958 logloss:23.9502\n",
            "[INFO random_forest.cc:796] Training of tree  81/1000 (tree index:80) done accuracy:0.287958 logloss:23.9573\n",
            "[INFO random_forest.cc:796] Training of tree  91/1000 (tree index:91) done accuracy:0.287958 logloss:23.9649\n",
            "[INFO random_forest.cc:796] Training of tree  101/1000 (tree index:101) done accuracy:0.287958 logloss:23.9686\n",
            "[INFO random_forest.cc:796] Training of tree  111/1000 (tree index:110) done accuracy:0.287958 logloss:23.9734\n",
            "[INFO random_forest.cc:796] Training of tree  121/1000 (tree index:120) done accuracy:0.287958 logloss:23.1257\n",
            "[INFO random_forest.cc:796] Training of tree  131/1000 (tree index:130) done accuracy:0.287958 logloss:23.1328\n",
            "[INFO random_forest.cc:796] Training of tree  141/1000 (tree index:140) done accuracy:0.287958 logloss:23.1374\n",
            "[INFO random_forest.cc:796] Training of tree  151/1000 (tree index:150) done accuracy:0.287958 logloss:23.1422\n",
            "[INFO random_forest.cc:796] Training of tree  161/1000 (tree index:160) done accuracy:0.287958 logloss:23.149\n",
            "[INFO random_forest.cc:796] Training of tree  171/1000 (tree index:171) done accuracy:0.287958 logloss:23.1534\n",
            "[INFO random_forest.cc:796] Training of tree  181/1000 (tree index:180) done accuracy:0.287958 logloss:23.1576\n",
            "[INFO random_forest.cc:796] Training of tree  191/1000 (tree index:190) done accuracy:0.287958 logloss:23.1615\n",
            "[INFO random_forest.cc:796] Training of tree  201/1000 (tree index:201) done accuracy:0.287958 logloss:23.1654\n",
            "[INFO random_forest.cc:796] Training of tree  211/1000 (tree index:210) done accuracy:0.287958 logloss:23.1678\n",
            "[INFO random_forest.cc:796] Training of tree  221/1000 (tree index:220) done accuracy:0.287958 logloss:23.1712\n",
            "[INFO random_forest.cc:796] Training of tree  231/1000 (tree index:230) done accuracy:0.287958 logloss:23.1745\n",
            "[INFO random_forest.cc:796] Training of tree  241/1000 (tree index:240) done accuracy:0.287958 logloss:23.1775\n",
            "[INFO random_forest.cc:796] Training of tree  251/1000 (tree index:250) done accuracy:0.287958 logloss:23.1807\n",
            "[INFO random_forest.cc:796] Training of tree  261/1000 (tree index:260) done accuracy:0.287958 logloss:23.1841\n",
            "[INFO random_forest.cc:796] Training of tree  271/1000 (tree index:270) done accuracy:0.287958 logloss:23.1869\n",
            "[INFO random_forest.cc:796] Training of tree  281/1000 (tree index:280) done accuracy:0.287958 logloss:23.1899\n",
            "[INFO random_forest.cc:796] Training of tree  291/1000 (tree index:290) done accuracy:0.287958 logloss:23.193\n",
            "[INFO random_forest.cc:796] Training of tree  301/1000 (tree index:300) done accuracy:0.287958 logloss:22.3747\n",
            "[INFO random_forest.cc:796] Training of tree  311/1000 (tree index:310) done accuracy:0.287958 logloss:22.3781\n",
            "[INFO random_forest.cc:796] Training of tree  321/1000 (tree index:321) done accuracy:0.287958 logloss:22.3813\n",
            "[INFO random_forest.cc:796] Training of tree  331/1000 (tree index:331) done accuracy:0.287958 logloss:22.191\n",
            "[INFO random_forest.cc:796] Training of tree  341/1000 (tree index:341) done accuracy:0.287958 logloss:21.8518\n",
            "[INFO random_forest.cc:796] Training of tree  351/1000 (tree index:350) done accuracy:0.287958 logloss:21.8543\n",
            "[INFO random_forest.cc:796] Training of tree  361/1000 (tree index:360) done accuracy:0.287958 logloss:21.8572\n",
            "[INFO random_forest.cc:796] Training of tree  371/1000 (tree index:370) done accuracy:0.287958 logloss:21.8602\n",
            "[INFO random_forest.cc:796] Training of tree  381/1000 (tree index:380) done accuracy:0.287958 logloss:21.8632\n",
            "[INFO random_forest.cc:796] Training of tree  391/1000 (tree index:390) done accuracy:0.287958 logloss:21.8667\n",
            "[INFO random_forest.cc:796] Training of tree  401/1000 (tree index:400) done accuracy:0.287958 logloss:21.8696\n",
            "[INFO random_forest.cc:796] Training of tree  411/1000 (tree index:410) done accuracy:0.287958 logloss:21.8723\n",
            "[INFO random_forest.cc:796] Training of tree  421/1000 (tree index:420) done accuracy:0.287958 logloss:21.8751\n",
            "[INFO random_forest.cc:796] Training of tree  431/1000 (tree index:430) done accuracy:0.287958 logloss:21.8775\n",
            "[INFO random_forest.cc:796] Training of tree  441/1000 (tree index:440) done accuracy:0.287958 logloss:21.8803\n",
            "[INFO random_forest.cc:796] Training of tree  451/1000 (tree index:450) done accuracy:0.287958 logloss:21.8832\n",
            "[INFO random_forest.cc:796] Training of tree  461/1000 (tree index:460) done accuracy:0.287958 logloss:21.3856\n",
            "[INFO random_forest.cc:796] Training of tree  471/1000 (tree index:470) done accuracy:0.287958 logloss:21.3881\n",
            "[INFO random_forest.cc:796] Training of tree  481/1000 (tree index:480) done accuracy:0.287958 logloss:21.3909\n",
            "[INFO random_forest.cc:796] Training of tree  491/1000 (tree index:491) done accuracy:0.287958 logloss:21.0572\n",
            "[INFO random_forest.cc:796] Training of tree  501/1000 (tree index:501) done accuracy:0.287958 logloss:21.0602\n",
            "[INFO random_forest.cc:796] Training of tree  511/1000 (tree index:511) done accuracy:0.287958 logloss:21.0629\n",
            "[INFO random_forest.cc:796] Training of tree  521/1000 (tree index:521) done accuracy:0.287958 logloss:21.0661\n",
            "[INFO random_forest.cc:796] Training of tree  531/1000 (tree index:531) done accuracy:0.287958 logloss:21.0691\n",
            "[INFO random_forest.cc:796] Training of tree  541/1000 (tree index:541) done accuracy:0.287958 logloss:20.4101\n",
            "[INFO random_forest.cc:796] Training of tree  551/1000 (tree index:551) done accuracy:0.287958 logloss:20.4129\n",
            "[INFO random_forest.cc:796] Training of tree  561/1000 (tree index:561) done accuracy:0.287958 logloss:20.416\n",
            "[INFO random_forest.cc:796] Training of tree  571/1000 (tree index:571) done accuracy:0.287958 logloss:20.4188\n",
            "[INFO random_forest.cc:796] Training of tree  581/1000 (tree index:581) done accuracy:0.287958 logloss:20.4215\n",
            "[INFO random_forest.cc:796] Training of tree  591/1000 (tree index:591) done accuracy:0.287958 logloss:20.4241\n",
            "[INFO random_forest.cc:796] Training of tree  601/1000 (tree index:601) done accuracy:0.287958 logloss:20.427\n",
            "[INFO random_forest.cc:796] Training of tree  611/1000 (tree index:611) done accuracy:0.287958 logloss:20.43\n",
            "[INFO random_forest.cc:796] Training of tree  621/1000 (tree index:621) done accuracy:0.287958 logloss:20.4331\n",
            "[INFO random_forest.cc:796] Training of tree  631/1000 (tree index:631) done accuracy:0.287958 logloss:20.4355\n",
            "[INFO random_forest.cc:796] Training of tree  641/1000 (tree index:640) done accuracy:0.287958 logloss:20.4381\n",
            "[INFO random_forest.cc:796] Training of tree  651/1000 (tree index:650) done accuracy:0.287958 logloss:20.4404\n",
            "[INFO random_forest.cc:796] Training of tree  661/1000 (tree index:660) done accuracy:0.287958 logloss:20.4426\n",
            "[INFO random_forest.cc:796] Training of tree  671/1000 (tree index:670) done accuracy:0.287958 logloss:20.2727\n",
            "[INFO random_forest.cc:796] Training of tree  681/1000 (tree index:680) done accuracy:0.287958 logloss:20.2751\n",
            "[INFO random_forest.cc:796] Training of tree  691/1000 (tree index:690) done accuracy:0.287958 logloss:20.2776\n",
            "[INFO random_forest.cc:796] Training of tree  701/1000 (tree index:700) done accuracy:0.287958 logloss:20.2799\n",
            "[INFO random_forest.cc:796] Training of tree  711/1000 (tree index:710) done accuracy:0.287958 logloss:20.2822\n",
            "[INFO random_forest.cc:796] Training of tree  721/1000 (tree index:720) done accuracy:0.287958 logloss:20.2847\n",
            "[INFO random_forest.cc:796] Training of tree  731/1000 (tree index:730) done accuracy:0.287958 logloss:20.287\n",
            "[INFO random_forest.cc:796] Training of tree  741/1000 (tree index:740) done accuracy:0.287958 logloss:20.2892\n",
            "[INFO random_forest.cc:796] Training of tree  751/1000 (tree index:750) done accuracy:0.287958 logloss:20.2918\n",
            "[INFO random_forest.cc:796] Training of tree  761/1000 (tree index:760) done accuracy:0.287958 logloss:20.2941\n",
            "[INFO random_forest.cc:796] Training of tree  771/1000 (tree index:770) done accuracy:0.287958 logloss:20.2761\n",
            "[INFO random_forest.cc:796] Training of tree  781/1000 (tree index:780) done accuracy:0.287958 logloss:20.2784\n",
            "[INFO random_forest.cc:796] Training of tree  791/1000 (tree index:790) done accuracy:0.287958 logloss:20.281\n",
            "[INFO random_forest.cc:796] Training of tree  801/1000 (tree index:800) done accuracy:0.287958 logloss:20.2829\n",
            "[INFO random_forest.cc:796] Training of tree  811/1000 (tree index:810) done accuracy:0.287958 logloss:20.2853\n",
            "[INFO random_forest.cc:796] Training of tree  821/1000 (tree index:820) done accuracy:0.287958 logloss:20.2873\n",
            "[INFO random_forest.cc:796] Training of tree  831/1000 (tree index:830) done accuracy:0.287958 logloss:20.2892\n",
            "[INFO random_forest.cc:796] Training of tree  841/1000 (tree index:840) done accuracy:0.287958 logloss:20.2911\n",
            "[INFO random_forest.cc:796] Training of tree  851/1000 (tree index:852) done accuracy:0.287958 logloss:20.2935\n",
            "[INFO random_forest.cc:796] Training of tree  861/1000 (tree index:861) done accuracy:0.287958 logloss:20.2953\n",
            "[INFO random_forest.cc:796] Training of tree  871/1000 (tree index:871) done accuracy:0.287958 logloss:20.2974\n",
            "[INFO random_forest.cc:796] Training of tree  881/1000 (tree index:881) done accuracy:0.287958 logloss:20.2993\n",
            "[INFO random_forest.cc:796] Training of tree  891/1000 (tree index:891) done accuracy:0.287958 logloss:20.3012\n",
            "[INFO random_forest.cc:796] Training of tree  901/1000 (tree index:901) done accuracy:0.287958 logloss:20.3033\n",
            "[INFO random_forest.cc:796] Training of tree  911/1000 (tree index:911) done accuracy:0.287958 logloss:20.3053\n",
            "[INFO random_forest.cc:796] Training of tree  921/1000 (tree index:921) done accuracy:0.287958 logloss:20.2751\n",
            "[INFO random_forest.cc:796] Training of tree  931/1000 (tree index:931) done accuracy:0.287958 logloss:20.2771\n",
            "[INFO random_forest.cc:796] Training of tree  941/1000 (tree index:942) done accuracy:0.287958 logloss:20.2791\n",
            "[INFO random_forest.cc:796] Training of tree  951/1000 (tree index:952) done accuracy:0.287958 logloss:20.281\n",
            "[INFO random_forest.cc:796] Training of tree  961/1000 (tree index:962) done accuracy:0.287958 logloss:20.2827\n",
            "[INFO random_forest.cc:796] Training of tree  971/1000 (tree index:972) done accuracy:0.287958 logloss:20.2846\n",
            "[INFO random_forest.cc:796] Training of tree  981/1000 (tree index:982) done accuracy:0.287958 logloss:20.2863\n",
            "[INFO random_forest.cc:796] Training of tree  991/1000 (tree index:992) done accuracy:0.287958 logloss:20.2881\n",
            "[INFO random_forest.cc:796] Training of tree  1000/1000 (tree index:933) done accuracy:0.287958 logloss:20.2899\n",
            "[INFO random_forest.cc:876] Final OOB metrics: accuracy:0.287958 logloss:20.2899\n",
            "[INFO kernel.cc:961] Export model in log directory: /tmp/tmpm782msh0 with prefix d8b658e47da74da7\n",
            "[INFO kernel.cc:978] Save model in resources\n",
            "[INFO kernel.cc:1176] Loading model from path /tmp/tmpm782msh0/model/ with prefix d8b658e47da74da7\n",
            "[WARNING utils.cc:73] The model does not have any input features i.e. the model is constant and will always return the same prediction.\n",
            "[INFO decision_forest.cc:639] Model loaded with 1000 root(s), 1000 node(s), and 0 input feature(s).\n",
            "[INFO abstract_model.cc:1248] Engine \"RandomForestGeneric\" built\n",
            "[INFO kernel.cc:1022] Use fast generic engine\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model trained in 0:00:00.207379\n",
            "Compiling model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7f7d659a0710> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7f7d659a0710> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: could not get source code\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "Model compiled.\n",
            "dict_items([('num_examples', [191]), ('accuracy', [0.2879581151832461]), ('loss', [20.289883240294298])])\n"
          ]
        }
      ],
      "source": [
        "%set_cell_height 300\n",
        "\n",
        "# Specify the model.\n",
        "model_1 = tfdf.keras.RandomForestModel(num_trees=1000, verbose=2)\n",
        "\n",
        "# Train the model.\n",
        "history = model_1.fit(x=train_ds)\n",
        "\n",
        "print(history.history.items())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9FMFGzwiHCt"
      },
      "source": [
        "In the previous logs, note that `sentence` is a `CATEGORICAL_SET` feature.\n",
        "\n",
        "The model is evaluated as usual:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpf-wHl094S1",
        "outputId": "e25ae660-c306-42c5-fb76-99fce2b5ae18",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n",
            "[0.0, 0.0]\n",
            "loss: 0.0\n",
            "Accuracy: 0.0\n"
          ]
        }
      ],
      "source": [
        "model_1.compile(metrics=[\"accuracy\"])\n",
        "evaluation = model_1.evaluate(test_ds)\n",
        "print(evaluation)\n",
        "print(f\"loss: {evaluation[0]}\")\n",
        "print(f\"Accuracy: {evaluation[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YliBX4GtjncQ"
      },
      "source": [
        "The training logs looks are follow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "OnTTtBNmjpo7",
        "outputId": "04d44112-42e2-44e1-f8ad-66eef1279eff",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logs = model_1.make_inspector().training_logs()\n",
        "plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.ylabel(\"Out-of-bag accuracy\")\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4qJ0ig3kgic"
      },
      "source": [
        "More trees would probably be beneficial (I am sure of it because I tried :p)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iil_oyOhCNx6"
      },
      "source": [
        "## Use a pretrained text embedding\n",
        "\n",
        "The previous example trained a Random Forest using raw text features. This example will use a pre-trained TF-Hub embedding to convert text features into a dense embedding, and then train a Random Forest on top of it. In this situation, the Random Forest will only \"see\" the numerical output of the embedding (i.e. it will not see the raw text). \n",
        "\n",
        "In this experiment,  will use the [Universal-Sentence-Encoder](https://tfhub.dev/google/universal-sentence-encoder/4). Different pre-trained embeddings might be suited for different types of text (e.g. different language, different task) but also for other type of structured features (e.g. images).\n",
        "\n",
        "**Note:** This embedding is large (1GB) and therefore the final model will be slow to run (compared to classical decision tree inference).\n",
        "\n",
        "The embedding module can be applied in one of two places:\n",
        "\n",
        "1. During the dataset preparation.\n",
        "2. In the pre-processing stage of the model.\n",
        "\n",
        "The second option is often preferable: Packaging the embedding in the model makes the model easier to use (and harder to misuse).\n",
        "\n",
        "First install TF-Hub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfYGXim_DskC",
        "outputId": "0fcb2e75-f509-4a22-8e57-94ab7201412e",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (1.21.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade tensorflow-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNSEhJgjEXww"
      },
      "source": [
        "Unlike before, you don't need to tokenize the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pS5SYqoScbOc",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(example):\n",
        "  label = (example[\"label\"] + 1) // 2\n",
        "  return {\"sentence\" : example[\"sentence\"]}, label\n",
        "\n",
        "# train_ds = all_ds[\"train\"].batch(100).map(prepare_dataset)\n",
        "# test_ds = all_ds[\"validation\"].batch(100).map(prepare_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "zHEsd8q_ESpC",
        "outputId": "2cef18f8-f670-4fe4-86e7-67b0b9f720ae",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Use /tmp/tmpxw87ii01 as temporary training directory\n",
            "Reading training dataset...\n",
            "Training dataset read in 0:00:06.610994. Found 191 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:03.598509\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "dict_items([('num_examples', [191]), ('accuracy', [0.4607329842931937]), ('loss', [1.6176925462265914])])\n"
          ]
        }
      ],
      "source": [
        "%set_cell_height 300\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "# NNLM (https://tfhub.dev/google/nnlm-en-dim128/2) is also a good choice.\n",
        "hub_url = \"http://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "embedding = hub.KerasLayer(hub_url)\n",
        "\n",
        "sentence = tf.keras.layers.Input(shape=(), name=\"sentence\", dtype=tf.string)\n",
        "embedded_sentence = embedding(sentence)\n",
        "\n",
        "raw_inputs = {\"sentence\": sentence}\n",
        "processed_inputs = {\"embedded_sentence\": embedded_sentence}\n",
        "preprocessor = tf.keras.Model(inputs=raw_inputs, outputs=processed_inputs)\n",
        "\n",
        "model_2 = tfdf.keras.RandomForestModel(\n",
        "    preprocessing=preprocessor,\n",
        "    num_trees=1000)\n",
        "\n",
        "history = model_2.fit(x=train_ds)\n",
        "print(history.history.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPLoDqiFKY18",
        "outputId": "6e5c4351-b091-4301-acbf-2a4129819218",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 2s 43ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n",
            "[0.0, 0.0]\n",
            "BinaryCrossentropyloss: 0.0\n",
            "Accuracy: 0.0\n"
          ]
        }
      ],
      "source": [
        "model_2.compile(metrics=[\"accuracy\"])\n",
        "evaluation = model_2.evaluate(test_ds)\n",
        "print(evaluation)\n",
        "\n",
        "print(f\"BinaryCrossentropyloss: {evaluation[0]}\")\n",
        "print(f\"Accuracy: {evaluation[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPsD3LyaMLHm"
      },
      "source": [
        "Note that categorical sets represent text differently from a dense embedding, so it may be useful to use both strategies jointly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37AGJamzboZQ"
      },
      "source": [
        "## Train a decision tree and neural network together\n",
        "\n",
        "The previous example used a pre-trained Neural Network (NN) to \n",
        "process the text features before passing them to the Random Forest. This example will train both the Neural Network and the Random Forest from scratch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJIxGwwzMkFl"
      },
      "source": [
        "TF-DF's Decision Forests do not back-propagate gradients ([although this is the subject of ongoing research](https://arxiv.org/abs/2007.14761)). Therefore, the training happens in two stages:\n",
        "\n",
        "1. Train the neural-network as a standard classification task:\n",
        "\n",
        "```\n",
        "example → [Normalize] → [Neural Network*] → [classification head] → prediction\n",
        "*: Training.\n",
        "```\n",
        "\n",
        "2. Replace the Neural Network's head (the last layer and the soft-max) with a Random Forest. Train the Random Forest as usual:\n",
        "\n",
        "```\n",
        "example → [Normalize] → [Neural Network] → [Random Forest*] → prediction\n",
        "*: Training.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSIvuAhzbjWO"
      },
      "source": [
        "### Prepare the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ore7f6tgcOMh"
      },
      "source": [
        "### Build the models\n",
        "\n",
        "Next create the neural network model using [Keras' functional style](https://www.tensorflow.org/guide/keras/functional). \n",
        "\n",
        "Bring back that Functional model from BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "S1Jfe4YteBqY",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "\n",
        "# Vocabulary size and number of words in a sequence.\n",
        "vocab_size = 100000\n",
        "sequence_length = 1000\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Note that the layer uses the custom standardization defined above.\n",
        "# Set maximum_sequence length as all samples are not of the same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_ds = train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCoQljyhelau"
      },
      "source": [
        "Build the body of the neural network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "KzocgbYNsH6y",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "embedding_dim=768\n",
        "\n",
        "nn_model = Sequential([\n",
        "  vectorize_layer,\n",
        "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
        "  GlobalAveragePooling1D(),\n",
        "  Dense(384, activation=\"relu\"),\n",
        "  tf.keras.layers.Dropout(0.1),  Dense(192, activation=\"relu\"),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  Dense(16, activation='relu'),\n",
        "  Dense(len(class_names))\n",
        "])\n",
        "\n",
        "# y = tf.keras.layers.Concatenate()(nn_processed_inputs)\n",
        "# y = tf.keras.layers.Dense(16, activation=tf.nn.relu6)(nn_raw_inputs)\n",
        "# last_layer = tf.keras.layers.Dense(8, activation=tf.nn.relu, name=\"last\")(y)\n",
        "\n",
        "# \"3\" for the three label classes. If it were a binary classification, the\n",
        "# output dim would be 1.\n",
        "# classification_output = tf.keras.layers.Dense(12)(y)\n",
        "\n",
        "# nn_model = tf.keras.models.Model(nn_raw_inputs, classification_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPbRKf1CfIrj"
      },
      "source": [
        "This `nn_model` directly produces classification logits. \n",
        "\n",
        "Next create a decision forest model. This will operate on the high level features that the neural network extracts in the last layer before that classification head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fnpGNyTuXvH",
        "outputId": "e4e66709-0191-4cf7-f1e7-cc14b84b216b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Use /tmp/tmprr46dvez as temporary training directory\n"
          ]
        }
      ],
      "source": [
        "# To reduce the risk of mistakes, group both the decision forest and the\n",
        "# neural network in a single keras model.\n",
        "df_and_nn_model = tfdf.keras.RandomForestModel(preprocessing=nn_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trq07lvMudlz"
      },
      "source": [
        "### Train and evaluate the models\n",
        "\n",
        "The model will be trained in two stages. First train the neural network with its own classification head:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "h4OyUWKiupuF",
        "outputId": "51f8d7ee-09bb-42e9-b0e1-06bf1367bc28",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "39/39 [==============================] - 20s 479ms/step - loss: 2.5857 - accuracy: 0.0942 - val_loss: 2.5793 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "39/39 [==============================] - 18s 470ms/step - loss: 2.4384 - accuracy: 0.0733 - val_loss: 2.8175 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "39/39 [==============================] - 18s 470ms/step - loss: 2.3738 - accuracy: 0.0419 - val_loss: 3.0782 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "39/39 [==============================] - 18s 462ms/step - loss: 2.1955 - accuracy: 0.1414 - val_loss: 2.9529 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "39/39 [==============================] - 18s 470ms/step - loss: 1.9633 - accuracy: 0.2618 - val_loss: 2.9561 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "39/39 [==============================] - 18s 468ms/step - loss: 1.8035 - accuracy: 0.2827 - val_loss: 2.9897 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "39/39 [==============================] - 18s 465ms/step - loss: 1.7845 - accuracy: 0.2723 - val_loss: 3.0381 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "39/39 [==============================] - 18s 464ms/step - loss: 1.9708 - accuracy: 0.3037 - val_loss: 5.5186 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "39/39 [==============================] - 18s 465ms/step - loss: 1.5833 - accuracy: 0.3927 - val_loss: 6.9498 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "39/39 [==============================] - 18s 468ms/step - loss: 1.3711 - accuracy: 0.3874 - val_loss: 6.2036 - val_accuracy: 0.0000e+00\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization_1 (TextV  (None, 1000)             0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 1000, 768)         76800000  \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 768)              0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 384)               295296    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 384)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 192)               73920     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 192)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 16)                3088      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 12)                204       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 77,172,508\n",
            "Trainable params: 77,172,508\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "%set_cell_height 300\n",
        "\n",
        "nn_model.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(),\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=[\"accuracy\"])\n",
        "\n",
        "nn_model.fit(x=train_ds, validation_data=test_ds, epochs=10)\n",
        "nn_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2mgMZOpgMQp"
      },
      "source": [
        "The neural network layers are shared between the two models. So now that the neural network is trained the decision forest model will be fit to the trained output of the neural network layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "JAc9niXqud7V",
        "outputId": "8dfae2bd-6589-4ae2-ae24-a6bd69297668",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading training dataset...\n",
            "Training dataset read in 0:00:00.192464. Found 191 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:00.112127\n",
            "Compiling model...\n",
            "Model compiled.\n",
            "{'num_examples': [191], 'accuracy': [0.8952879581151832], 'loss': [0.470283001992871]}\n"
          ]
        }
      ],
      "source": [
        "%set_cell_height 300\n",
        "\n",
        "history = df_and_nn_model.fit(x=train_ds)\n",
        "\n",
        "print(history.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF8Ru2HSv1a5"
      },
      "source": [
        "Now evaluate the composed model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPMlcObzuw89",
        "outputId": "0d15034f-d35e-4013-e9a9-eb903ebc9fb9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n",
            "Evaluation: [0.0, 0.0]\n"
          ]
        }
      ],
      "source": [
        "df_and_nn_model.compile(metrics=[\"accuracy\"])\n",
        "print(\"Evaluation:\", df_and_nn_model.evaluate(test_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awiHEznlv5sI"
      },
      "source": [
        "Compare it to the Neural Network alone:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--ompWYTvxM-",
        "outputId": "6992e630-36bb-495c-dda9-81d6596202d6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 0s 7ms/step - loss: 6.2036 - accuracy: 0.0000e+00\n",
            "Evaluation : [6.203577995300293, 0.0]\n"
          ]
        }
      ],
      "source": [
        "print(\"Evaluation :\", nn_model.evaluate(test_ds))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "intermediate_colab.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
