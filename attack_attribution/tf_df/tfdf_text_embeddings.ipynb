{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yo62ffS5TF5"
      },
      "source": [
        "# Using text and neural network features\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/decision_forests/tutorials/intermediate_colab\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/decision-forests/blob/main/documentation/tutorials/intermediate_colab.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/decision-forests/blob/main/documentation/tutorials/intermediate_colab.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/decision-forests/documentation/tutorials/intermediate_colab.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://tfhub.dev/google/universal-sentence-encoder/4\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub model</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrCwCCxhiAL7"
      },
      "source": [
        "Welcome to the **Intermediate Colab** for **TensorFlow Decision Forests** (**TF-DF**).\n",
        "In this colab, you will learn about some more advanced capabilities of **TF-DF**, including how to deal with natural language features.\n",
        "\n",
        "This colab assumes you are familiar with the concepts presented the [Beginner colab](beginner_colab.ipynb), notably about the installation about TF-DF.\n",
        "\n",
        "In this colab, you will:\n",
        "\n",
        "1. Train a Random Forest that consumes text features natively as categorical sets.\n",
        "\n",
        "1. Train a Random Forest that consumes text features using a [TensorFlow Hub](https://www.tensorflow.org/hub) module. In this setting (transfer learning), the module is already pre-trained on a large text corpus.\n",
        "\n",
        "1. Train a Gradient Boosted Decision Trees (GBDT) and a Neural Network together. The GBDT will consume the output of the Neural Network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzskapxq7gdo"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mZiInVYfffAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8182c4f1-b7b7-4054-a404-787298f9e050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_decision_forests\n",
            "  Downloading tensorflow_decision_forests-0.2.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.0 MB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (1.21.6)\n",
            "Collecting tensorflow~=2.9.1\n",
            "  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 6.3 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (1.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (1.3.5)\n",
            "Collecting wurlitzer\n",
            "  Downloading wurlitzer-3.0.2-py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from tensorflow_decision_forests) (0.37.1)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 44.8 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (0.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (21.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (3.17.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (1.47.0)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (1.1.2)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 39.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (1.6.3)\n",
            "Collecting flatbuffers<2,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (0.26.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.9.1->tensorflow_decision_forests) (14.0.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.9.1->tensorflow_decision_forests) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (3.4.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow~=2.9.1->tensorflow_decision_forests) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow~=2.9.1->tensorflow_decision_forests) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tensorflow_decision_forests) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tensorflow_decision_forests) (2.8.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, flatbuffers, wurlitzer, tensorflow, tensorflow-decision-forests\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "Successfully installed flatbuffers-1.12 gast-0.4.0 keras-2.9.0 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-decision-forests-0.2.7 tensorflow-estimator-2.9.0 wurlitzer-3.0.2\n"
          ]
        }
      ],
      "source": [
        "# Install TensorFlow Dececision Forests\n",
        "!pip install tensorflow_decision_forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EFndCFdoJM5"
      },
      "source": [
        "[Wurlitzer](https://pypi.org/project/wurlitzer/) is needed to display the detailed training logs in Colabs (when using `verbose=2` in the model constructor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L06XWRdSoLj5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71a02714-b9d4-4fc0-8402-fda1f4c64afc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.7/dist-packages (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wurlitzer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7PlfbnxYcPf"
      },
      "source": [
        "Import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RsCV2oAS7gC_"
      },
      "outputs": [],
      "source": [
        "import tensorflow_decision_forests as tfdf\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2fsI0y5x5i5"
      },
      "source": [
        "The hidden code cell limits the output height in colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jZXB4o6Tlu0i"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "from IPython.core.magic import register_line_magic\n",
        "from IPython.display import Javascript\n",
        "from IPython.display import display as ipy_display\n",
        "\n",
        "# Some of the model training logs can cover the full\n",
        "# screen if not compressed to a smaller viewport.\n",
        "# This magic allows setting a max height for a cell.\n",
        "@register_line_magic\n",
        "def set_cell_height(size):\n",
        "  ipy_display(\n",
        "      Javascript(\"google.colab.output.setIframeHeight(0, true, {maxHeight: \" +\n",
        "                 str(size) + \"})\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_D4Ft4o65XT"
      },
      "source": [
        "## Use raw text as features\n",
        "\n",
        "TF-DF can consume [categorical-set](https://arxiv.org/pdf/2009.09991.pdf) features natively. Categorical-sets represent text features as bags of words (or n-grams).\n",
        "\n",
        "For example: `\"The little blue dog\" ` → `{\"the\", \"little\", \"blue\", \"dog\"}`\n",
        "\n",
        "In this example, you'll will train a Random Forest on the [Threat Intelligence Reports](https://github.com/threatDataset) dataset. The objective of this dataset is to classify reports into 12 threat actors. You'll will use the multiclass classification version of the dataset.\n",
        "\n",
        "**Note:** Categorical-set features can be expensive to train. In this colab, we will train a small Random Forest with 1000 trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uVN-j0E4Q1T3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6591c332-ae40-4699-d491-513efdf172a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 238 files belonging to 12 classes.\n",
            "Using 191 files for training.\n",
            "Found 238 files belonging to 12 classes.\n",
            "Using 47 files for validation.\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "\n",
        "url = 'https://github.com/eyalmazuz/AttackAttributionDataset/archive/refs/heads/master.zip' \n",
        "\n",
        "dataset = tf.keras.utils.get_file('master.zip', url, extract=True, cache_dir='.', cache_subdir='')\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "batch_size = 5\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'AttackAttributionDataset-master',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    shuffle=False,\n",
        "    seed=seed)\n",
        "\n",
        "class_names = raw_train_ds.class_names\n",
        "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'AttackAttributionDataset-master',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    shuffle=False,\n",
        "    subset='validation',\n",
        "    seed=seed)\n",
        "\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "# Display the first 3 examples of the test fold.\n",
        "# for example in train_ds.take(1):\n",
        "#   print({attr_name: attr_tensor.numpy() for attr_name, attr_tensor in example.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHiQUWE2XDYN"
      },
      "source": [
        "The dataset is modified as follows:\n",
        "\n",
        "1. The raw labels are integers in `{-1, 1}`, but the learning algorithm expects positive integer labels e.g. `{0, 1}`. Therefore, the labels are transformed as follows: `new_labels = (original_labels + 1) / 2`.\n",
        "1. A batch-size of 64 is applied to make reading the dataset more efficient.\n",
        "1. The `sentence` attribute needs to be tokenized, i.e. `\"hello world\" -> [\"hello\", \"world\"]`.\n",
        "\n",
        "\n",
        "**Note:** This example doesn't use the `test` split of the dataset as it does not have labels. If `test` split had labels, you could concatenate the `validation` fold into the `train` one (e.g. `all_ds[\"train\"].concatenate(all_ds[\"validation\"])`).\n",
        "\n",
        "**Details:** Some decision forest learning algorithms do not need a validation dataset (e.g. Random Forests) while others do (e.g. Gradient Boosted Trees in some cases). Since each learning algorithm under TF-DF can use validation data differently, TF-DF handles train/validation splits internally. As a result, when you have a training and validation sets, they can always be concatenated as input to the learning algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yqYDKTKdSPYw"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(example):\n",
        "  label = (example[\"label\"] + 1) // 2\n",
        "  return {\"sentence\" : tf.strings.split(example[\"sentence\"])}, label\n",
        "\n",
        "# train_ds = all_ds[\"train\"].batch(100).map(prepare_dataset)\n",
        "# test_ds = all_ds[\"validation\"].batch(100).map(prepare_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYkIjROI9w43"
      },
      "source": [
        "Finaly, train and evaluate the model as usual. TF-DF  automatically detects multi-valued categorical features as categorical-set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "mpxTtYo39wYZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "b27a1978-75f1-4636-9d78-daa61bfafaa6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use 4 thread(s) for training\n",
            "Use /tmp/tmp2qge13_e as temporary training directory\n",
            "Reading training dataset...\n",
            "Training tensor examples:\n",
            "Features: Tensor(\"data:0\", shape=(None,), dtype=string)\n",
            "Label: Tensor(\"data_1:0\", shape=(None,), dtype=int32)\n",
            "Weights: None\n",
            "Normalized tensor features:\n",
            " {'data:0': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data:0' shape=(None,) dtype=string>)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function CoreModel._consumes_training_examples_until_eof at 0x7ff2407f0320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset read in 0:00:00.455230. Found 191 examples.\n",
            "Training model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO kernel.cc:813] Start Yggdrasil model training\n",
            "[INFO kernel.cc:814] Collect training examples\n",
            "[INFO kernel.cc:422] Number of batches: 39\n",
            "[INFO kernel.cc:423] Number of examples: 191\n",
            "[INFO data_spec_inference.cc:303] 190 item(s) have been pruned (i.e. they are considered out of dictionary) for the column data:0 (0 item(s) left) because min_value_count=5 and max_number_of_unique_values=2000\n",
            "[INFO kernel.cc:836] Training dataset:\n",
            "Number of records: 191\n",
            "Number of columns: 2\n",
            "\n",
            "Number of columns by type:\n",
            "\tCATEGORICAL: 2 (100%)\n",
            "\n",
            "Columns:\n",
            "\n",
            "CATEGORICAL: 2 (100%)\n",
            "\t0: \"data:0\" CATEGORICAL has-dict vocab-size:1 num-oods:190 (99.4764%)\n",
            "\t1: \"__LABEL\" CATEGORICAL integerized vocab-size:10 no-ood-item\n",
            "\n",
            "Terminology:\n",
            "\tnas: Number of non-available (i.e. missing) values.\n",
            "\tood: Out of dictionary.\n",
            "\tmanually-defined: Attribute which type is manually defined by the user i.e. the type was not automatically inferred.\n",
            "\ttokenized: The attribute value is obtained through tokenization.\n",
            "\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n",
            "\tvocab-size: Number of unique values.\n",
            "\n",
            "[INFO kernel.cc:882] Configure learner\n",
            "[INFO kernel.cc:912] Training config:\n",
            "learner: \"RANDOM_FOREST\"\n",
            "features: \"data:0\"\n",
            "label: \"__LABEL\"\n",
            "task: CLASSIFICATION\n",
            "random_seed: 123456\n",
            "metadata {\n",
            "  framework: \"TF Keras\"\n",
            "}\n",
            "pure_serving_model: false\n",
            "[yggdrasil_decision_forests.model.random_forest.proto.random_forest_config] {\n",
            "  num_trees: 1000\n",
            "  decision_tree {\n",
            "    max_depth: 16\n",
            "    min_examples: 5\n",
            "    in_split_min_examples_check: true\n",
            "    keep_non_leaf_label_distribution: true\n",
            "    num_candidate_attributes: 0\n",
            "    missing_value_policy: GLOBAL_IMPUTATION\n",
            "    allow_na_conditions: false\n",
            "    categorical_set_greedy_forward {\n",
            "      sampling: 0.1\n",
            "      max_num_items: -1\n",
            "      min_item_frequency: 1\n",
            "    }\n",
            "    growing_strategy_local {\n",
            "    }\n",
            "    categorical {\n",
            "      cart {\n",
            "      }\n",
            "    }\n",
            "    axis_aligned_split {\n",
            "    }\n",
            "    internal {\n",
            "      sorting_strategy: PRESORTED\n",
            "    }\n",
            "    uplift {\n",
            "      min_examples_in_treatment: 5\n",
            "      split_score: KULLBACK_LEIBLER\n",
            "    }\n",
            "  }\n",
            "  winner_take_all_inference: true\n",
            "  compute_oob_performances: true\n",
            "  compute_oob_variable_importances: false\n",
            "  num_oob_variable_importances_permutations: 1\n",
            "  bootstrap_training_dataset: true\n",
            "  bootstrap_size_ratio: 1\n",
            "  adapt_bootstrap_size_ratio_for_maximum_training_duration: false\n",
            "  sampling_with_replacement: true\n",
            "}\n",
            "\n",
            "[INFO kernel.cc:915] Deployment config:\n",
            "cache_path: \"/tmp/tmp2qge13_e/working_cache\"\n",
            "num_threads: 4\n",
            "try_resume_training: true\n",
            "\n",
            "[INFO kernel.cc:944] Train model\n",
            "[INFO random_forest.cc:407] Training random forest on 191 example(s) and 1 feature(s).\n",
            "[INFO random_forest.cc:796] Training of tree  1/1000 (tree index:0) done accuracy:0.209677 logloss:28.4861\n",
            "[INFO random_forest.cc:796] Training of tree  11/1000 (tree index:10) done accuracy:0.284211 logloss:25.7997\n",
            "[INFO random_forest.cc:796] Training of tree  21/1000 (tree index:20) done accuracy:0.287958 logloss:23.9055\n",
            "[INFO random_forest.cc:796] Training of tree  31/1000 (tree index:30) done accuracy:0.287958 logloss:23.9155\n",
            "[INFO random_forest.cc:796] Training of tree  41/1000 (tree index:40) done accuracy:0.287958 logloss:23.9282\n",
            "[INFO random_forest.cc:796] Training of tree  51/1000 (tree index:50) done accuracy:0.287958 logloss:23.9371\n",
            "[INFO random_forest.cc:796] Training of tree  61/1000 (tree index:60) done accuracy:0.287958 logloss:23.946\n",
            "[INFO random_forest.cc:796] Training of tree  71/1000 (tree index:70) done accuracy:0.287958 logloss:23.9514\n",
            "[INFO random_forest.cc:796] Training of tree  81/1000 (tree index:80) done accuracy:0.287958 logloss:23.9588\n",
            "[INFO random_forest.cc:796] Training of tree  91/1000 (tree index:90) done accuracy:0.287958 logloss:23.9652\n",
            "[INFO random_forest.cc:796] Training of tree  101/1000 (tree index:100) done accuracy:0.287958 logloss:23.9685\n",
            "[INFO random_forest.cc:796] Training of tree  111/1000 (tree index:110) done accuracy:0.287958 logloss:23.9731\n",
            "[INFO random_forest.cc:796] Training of tree  121/1000 (tree index:120) done accuracy:0.287958 logloss:23.1261\n",
            "[INFO random_forest.cc:796] Training of tree  131/1000 (tree index:130) done accuracy:0.287958 logloss:23.1319\n",
            "[INFO random_forest.cc:796] Training of tree  141/1000 (tree index:140) done accuracy:0.287958 logloss:23.1378\n",
            "[INFO random_forest.cc:796] Training of tree  151/1000 (tree index:150) done accuracy:0.287958 logloss:23.1425\n",
            "[INFO random_forest.cc:796] Training of tree  161/1000 (tree index:160) done accuracy:0.287958 logloss:23.1487\n",
            "[INFO random_forest.cc:796] Training of tree  171/1000 (tree index:170) done accuracy:0.287958 logloss:23.1531\n",
            "[INFO random_forest.cc:796] Training of tree  181/1000 (tree index:180) done accuracy:0.287958 logloss:23.1568\n",
            "[INFO random_forest.cc:796] Training of tree  191/1000 (tree index:190) done accuracy:0.287958 logloss:23.1606\n",
            "[INFO random_forest.cc:796] Training of tree  201/1000 (tree index:200) done accuracy:0.287958 logloss:23.1647\n",
            "[INFO random_forest.cc:796] Training of tree  211/1000 (tree index:210) done accuracy:0.287958 logloss:23.1679\n",
            "[INFO random_forest.cc:796] Training of tree  221/1000 (tree index:220) done accuracy:0.287958 logloss:22.3431\n",
            "[INFO random_forest.cc:796] Training of tree  231/1000 (tree index:230) done accuracy:0.287958 logloss:22.3476\n",
            "[INFO random_forest.cc:796] Training of tree  241/1000 (tree index:240) done accuracy:0.287958 logloss:22.3512\n",
            "[INFO random_forest.cc:796] Training of tree  251/1000 (tree index:251) done accuracy:0.287958 logloss:22.3567\n",
            "[INFO random_forest.cc:796] Training of tree  261/1000 (tree index:261) done accuracy:0.287958 logloss:22.361\n",
            "[INFO random_forest.cc:796] Training of tree  271/1000 (tree index:271) done accuracy:0.287958 logloss:22.3646\n",
            "[INFO random_forest.cc:796] Training of tree  281/1000 (tree index:281) done accuracy:0.287958 logloss:22.368\n",
            "[INFO random_forest.cc:796] Training of tree  291/1000 (tree index:291) done accuracy:0.287958 logloss:22.3719\n",
            "[INFO random_forest.cc:796] Training of tree  301/1000 (tree index:301) done accuracy:0.287958 logloss:22.3751\n",
            "[INFO random_forest.cc:796] Training of tree  311/1000 (tree index:311) done accuracy:0.287958 logloss:22.379\n",
            "[INFO random_forest.cc:796] Training of tree  321/1000 (tree index:322) done accuracy:0.287958 logloss:22.1887\n",
            "[INFO random_forest.cc:796] Training of tree  331/1000 (tree index:330) done accuracy:0.287958 logloss:22.1911\n",
            "[INFO random_forest.cc:796] Training of tree  341/1000 (tree index:341) done accuracy:0.287958 logloss:21.8515\n",
            "[INFO random_forest.cc:796] Training of tree  351/1000 (tree index:351) done accuracy:0.287958 logloss:21.8546\n",
            "[INFO random_forest.cc:796] Training of tree  361/1000 (tree index:361) done accuracy:0.287958 logloss:21.8573\n",
            "[INFO random_forest.cc:796] Training of tree  371/1000 (tree index:371) done accuracy:0.287958 logloss:21.8603\n",
            "[INFO random_forest.cc:796] Training of tree  381/1000 (tree index:381) done accuracy:0.287958 logloss:21.8637\n",
            "[INFO random_forest.cc:796] Training of tree  391/1000 (tree index:391) done accuracy:0.287958 logloss:21.8672\n",
            "[INFO random_forest.cc:796] Training of tree  401/1000 (tree index:401) done accuracy:0.287958 logloss:21.87\n",
            "[INFO random_forest.cc:796] Training of tree  411/1000 (tree index:411) done accuracy:0.287958 logloss:21.8728\n",
            "[INFO random_forest.cc:796] Training of tree  421/1000 (tree index:421) done accuracy:0.287958 logloss:21.8756\n",
            "[INFO random_forest.cc:796] Training of tree  431/1000 (tree index:431) done accuracy:0.287958 logloss:21.8779\n",
            "[INFO random_forest.cc:796] Training of tree  441/1000 (tree index:441) done accuracy:0.287958 logloss:21.8807\n",
            "[INFO random_forest.cc:796] Training of tree  451/1000 (tree index:451) done accuracy:0.287958 logloss:21.8836\n",
            "[INFO random_forest.cc:796] Training of tree  461/1000 (tree index:461) done accuracy:0.287958 logloss:21.3859\n",
            "[INFO random_forest.cc:796] Training of tree  471/1000 (tree index:472) done accuracy:0.287958 logloss:21.3889\n",
            "[INFO random_forest.cc:796] Training of tree  481/1000 (tree index:482) done accuracy:0.287958 logloss:21.0545\n",
            "[INFO random_forest.cc:796] Training of tree  491/1000 (tree index:492) done accuracy:0.287958 logloss:21.0576\n",
            "[INFO random_forest.cc:796] Training of tree  501/1000 (tree index:502) done accuracy:0.287958 logloss:21.0603\n",
            "[INFO random_forest.cc:796] Training of tree  511/1000 (tree index:511) done accuracy:0.287958 logloss:21.0629\n",
            "[INFO random_forest.cc:796] Training of tree  521/1000 (tree index:521) done accuracy:0.287958 logloss:21.0658\n",
            "[INFO random_forest.cc:796] Training of tree  531/1000 (tree index:531) done accuracy:0.287958 logloss:21.0689\n",
            "[INFO random_forest.cc:796] Training of tree  541/1000 (tree index:542) done accuracy:0.287958 logloss:21.0718\n",
            "[INFO random_forest.cc:796] Training of tree  551/1000 (tree index:550) done accuracy:0.287958 logloss:21.0738\n",
            "[INFO random_forest.cc:796] Training of tree  561/1000 (tree index:560) done accuracy:0.287958 logloss:21.076\n",
            "[INFO random_forest.cc:796] Training of tree  571/1000 (tree index:570) done accuracy:0.287958 logloss:21.0788\n",
            "[INFO random_forest.cc:796] Training of tree  581/1000 (tree index:580) done accuracy:0.287958 logloss:21.081\n",
            "[INFO random_forest.cc:796] Training of tree  591/1000 (tree index:590) done accuracy:0.287958 logloss:21.0833\n",
            "[INFO random_forest.cc:796] Training of tree  601/1000 (tree index:600) done accuracy:0.287958 logloss:21.0857\n",
            "[INFO random_forest.cc:796] Training of tree  611/1000 (tree index:610) done accuracy:0.287958 logloss:20.4305\n",
            "[INFO random_forest.cc:796] Training of tree  621/1000 (tree index:620) done accuracy:0.287958 logloss:20.4328\n",
            "[INFO random_forest.cc:796] Training of tree  631/1000 (tree index:630) done accuracy:0.287958 logloss:20.4353\n",
            "[INFO random_forest.cc:796] Training of tree  641/1000 (tree index:640) done accuracy:0.287958 logloss:20.4379\n",
            "[INFO random_forest.cc:796] Training of tree  651/1000 (tree index:650) done accuracy:0.287958 logloss:20.4405\n",
            "[INFO random_forest.cc:796] Training of tree  661/1000 (tree index:660) done accuracy:0.287958 logloss:20.4427\n",
            "[INFO random_forest.cc:796] Training of tree  671/1000 (tree index:670) done accuracy:0.287958 logloss:20.2728\n",
            "[INFO random_forest.cc:796] Training of tree  681/1000 (tree index:680) done accuracy:0.287958 logloss:20.2752\n",
            "[INFO random_forest.cc:796] Training of tree  691/1000 (tree index:690) done accuracy:0.287958 logloss:20.2777\n",
            "[INFO random_forest.cc:796] Training of tree  701/1000 (tree index:700) done accuracy:0.287958 logloss:20.2801\n",
            "[INFO random_forest.cc:796] Training of tree  711/1000 (tree index:710) done accuracy:0.287958 logloss:20.2824\n",
            "[INFO random_forest.cc:796] Training of tree  721/1000 (tree index:720) done accuracy:0.287958 logloss:20.2849\n",
            "[INFO random_forest.cc:796] Training of tree  731/1000 (tree index:730) done accuracy:0.287958 logloss:20.2872\n",
            "[INFO random_forest.cc:796] Training of tree  741/1000 (tree index:740) done accuracy:0.287958 logloss:20.2893\n",
            "[INFO random_forest.cc:796] Training of tree  751/1000 (tree index:750) done accuracy:0.287958 logloss:20.2917\n",
            "[INFO random_forest.cc:796] Training of tree  761/1000 (tree index:760) done accuracy:0.287958 logloss:20.2939\n",
            "[INFO random_forest.cc:796] Training of tree  771/1000 (tree index:770) done accuracy:0.287958 logloss:20.2762\n",
            "[INFO random_forest.cc:796] Training of tree  781/1000 (tree index:780) done accuracy:0.287958 logloss:20.2785\n",
            "[INFO random_forest.cc:796] Training of tree  791/1000 (tree index:790) done accuracy:0.287958 logloss:20.281\n",
            "[INFO random_forest.cc:796] Training of tree  801/1000 (tree index:800) done accuracy:0.287958 logloss:20.283\n",
            "[INFO random_forest.cc:796] Training of tree  811/1000 (tree index:811) done accuracy:0.287958 logloss:20.2859\n",
            "[INFO random_forest.cc:796] Training of tree  821/1000 (tree index:820) done accuracy:0.287958 logloss:20.2875\n",
            "[INFO random_forest.cc:796] Training of tree  831/1000 (tree index:830) done accuracy:0.287958 logloss:20.2892\n",
            "[INFO random_forest.cc:796] Training of tree  841/1000 (tree index:840) done accuracy:0.287958 logloss:20.2911\n",
            "[INFO random_forest.cc:796] Training of tree  851/1000 (tree index:850) done accuracy:0.287958 logloss:20.2931\n",
            "[INFO random_forest.cc:796] Training of tree  861/1000 (tree index:860) done accuracy:0.287958 logloss:20.2951\n",
            "[INFO random_forest.cc:796] Training of tree  871/1000 (tree index:870) done accuracy:0.287958 logloss:20.2972\n",
            "[INFO random_forest.cc:796] Training of tree  881/1000 (tree index:880) done accuracy:0.287958 logloss:20.2991\n",
            "[INFO random_forest.cc:796] Training of tree  891/1000 (tree index:890) done accuracy:0.287958 logloss:20.301\n",
            "[INFO random_forest.cc:796] Training of tree  901/1000 (tree index:900) done accuracy:0.287958 logloss:20.303\n",
            "[INFO random_forest.cc:796] Training of tree  911/1000 (tree index:910) done accuracy:0.287958 logloss:20.3051\n",
            "[INFO random_forest.cc:796] Training of tree  921/1000 (tree index:920) done accuracy:0.287958 logloss:20.275\n",
            "[INFO random_forest.cc:796] Training of tree  931/1000 (tree index:930) done accuracy:0.287958 logloss:20.2769\n",
            "[INFO random_forest.cc:796] Training of tree  941/1000 (tree index:940) done accuracy:0.287958 logloss:20.2787\n",
            "[INFO random_forest.cc:796] Training of tree  951/1000 (tree index:950) done accuracy:0.287958 logloss:20.2807\n",
            "[INFO random_forest.cc:796] Training of tree  961/1000 (tree index:960) done accuracy:0.287958 logloss:20.2824\n",
            "[INFO random_forest.cc:796] Training of tree  971/1000 (tree index:970) done accuracy:0.287958 logloss:20.2842\n",
            "[INFO random_forest.cc:796] Training of tree  981/1000 (tree index:980) done accuracy:0.287958 logloss:20.2861\n",
            "[INFO random_forest.cc:796] Training of tree  991/1000 (tree index:990) done accuracy:0.287958 logloss:20.2878\n",
            "[INFO random_forest.cc:796] Training of tree  1000/1000 (tree index:999) done accuracy:0.287958 logloss:20.2894\n",
            "[INFO random_forest.cc:876] Final OOB metrics: accuracy:0.287958 logloss:20.2894\n",
            "[INFO kernel.cc:961] Export model in log directory: /tmp/tmp2qge13_e with prefix 0ba3e8eb062249e6\n",
            "[INFO kernel.cc:978] Save model in resources\n",
            "[INFO kernel.cc:1176] Loading model from path /tmp/tmp2qge13_e/model/ with prefix 0ba3e8eb062249e6\n",
            "[WARNING utils.cc:73] The model does not have any input features i.e. the model is constant and will always return the same prediction.\n",
            "[INFO decision_forest.cc:639] Model loaded with 1000 root(s), 1000 node(s), and 0 input feature(s).\n",
            "[INFO kernel.cc:1022] Use fast generic engine\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained in 0:00:00.204409\n",
            "Compiling model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function CoreModel.make_predict_function.<locals>.predict_function_trained at 0x7ff1a91bb320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 6 calls to <function CoreModel.yggdrasil_model_path_tensor at 0x7ff1a9198710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('num_examples', [191]), ('accuracy', [0.2879581151832461]), ('loss', [20.289362650701083])])\n"
          ]
        }
      ],
      "source": [
        "%set_cell_height 300\n",
        "\n",
        "# Specify the model.\n",
        "model_1 = tfdf.keras.RandomForestModel(num_trees=1000, verbose=2)\n",
        "\n",
        "# Train the model.\n",
        "history = model_1.fit(x=train_ds)\n",
        "\n",
        "print(history.history.items())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9FMFGzwiHCt"
      },
      "source": [
        "In the previous logs, note that `sentence` is a `CATEGORICAL_SET` feature.\n",
        "\n",
        "The model is evaluated as usual:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "cpf-wHl094S1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d3d7eae-05f0-4349-81f4-4792434064c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n",
            "[0.0, 0.0]\n",
            "loss: 0.0\n",
            "Accuracy: 0.0\n"
          ]
        }
      ],
      "source": [
        "model_1.compile(metrics=[\"accuracy\"])\n",
        "evaluation = model_1.evaluate(test_ds)\n",
        "print(evaluation)\n",
        "print(f\"loss: {evaluation[0]}\")\n",
        "print(f\"Accuracy: {evaluation[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YliBX4GtjncQ"
      },
      "source": [
        "The training logs looks are follow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OnTTtBNmjpo7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "a80a2a93-3eba-485d-b2f4-36085ceac69c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logs = model_1.make_inspector().training_logs()\n",
        "plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.ylabel(\"Out-of-bag accuracy\")\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4qJ0ig3kgic"
      },
      "source": [
        "More trees would probably be beneficial (I am sure of it because I tried :p)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iil_oyOhCNx6"
      },
      "source": [
        "## Use a pretrained text embedding\n",
        "\n",
        "The previous example trained a Random Forest using raw text features. This example will use a pre-trained TF-Hub embedding to convert text features into a dense embedding, and then train a Random Forest on top of it. In this situation, the Random Forest will only \"see\" the numerical output of the embedding (i.e. it will not see the raw text). \n",
        "\n",
        "In this experiment,  will use the [Universal-Sentence-Encoder](https://tfhub.dev/google/universal-sentence-encoder/4). Different pre-trained embeddings might be suited for different types of text (e.g. different language, different task) but also for other type of structured features (e.g. images).\n",
        "\n",
        "**Note:** This embedding is large (1GB) and therefore the final model will be slow to run (compared to classical decision tree inference).\n",
        "\n",
        "The embedding module can be applied in one of two places:\n",
        "\n",
        "1. During the dataset preparation.\n",
        "2. In the pre-processing stage of the model.\n",
        "\n",
        "The second option is often preferable: Packaging the embedding in the model makes the model easier to use (and harder to misuse).\n",
        "\n",
        "First install TF-Hub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "QfYGXim_DskC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d024b55-9c48-4745-9ee2-e2d48db5f307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (1.21.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade tensorflow-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNSEhJgjEXww"
      },
      "source": [
        "Unlike before, you don't need to tokenize the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "pS5SYqoScbOc"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(example):\n",
        "  label = (example[\"label\"] + 1) // 2\n",
        "  return {\"sentence\" : example[\"sentence\"]}, label\n",
        "\n",
        "# train_ds = all_ds[\"train\"].batch(100).map(prepare_dataset)\n",
        "# test_ds = all_ds[\"validation\"].batch(100).map(prepare_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "zHEsd8q_ESpC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "4dfe5416-90a8-49aa-df85-955041baa443"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use /tmp/tmp2xmadvt5 as temporary training directory\n",
            "Reading training dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function CoreModel._consumes_training_examples_until_eof at 0x7ff2407f0320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset read in 0:00:17.204205. Found 191 examples.\n",
            "Training model...\n",
            "Model trained in 0:00:04.547459\n",
            "Compiling model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function CoreModel.make_predict_function.<locals>.predict_function_trained at 0x7ff1a8eb24d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 7 calls to <function CoreModel.yggdrasil_model_path_tensor at 0x7ff1aa66d170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('num_examples', [191]), ('accuracy', [0.4607329842931937]), ('loss', [1.618805227557402])])\n"
          ]
        }
      ],
      "source": [
        "%set_cell_height 300\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "# NNLM (https://tfhub.dev/google/nnlm-en-dim128/2) is also a good choice.\n",
        "hub_url = \"http://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "embedding = hub.KerasLayer(hub_url)\n",
        "\n",
        "sentence = tf.keras.layers.Input(shape=(), name=\"sentence\", dtype=tf.string)\n",
        "embedded_sentence = embedding(sentence)\n",
        "\n",
        "raw_inputs = {\"sentence\": sentence}\n",
        "processed_inputs = {\"embedded_sentence\": embedded_sentence}\n",
        "preprocessor = tf.keras.Model(inputs=raw_inputs, outputs=processed_inputs)\n",
        "\n",
        "model_2 = tfdf.keras.RandomForestModel(\n",
        "    preprocessing=preprocessor,\n",
        "    num_trees=1000)\n",
        "\n",
        "history = model_2.fit(x=train_ds)\n",
        "print(history.history.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "xPLoDqiFKY18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f176237-2c14-4847-e76b-1252ee2c4c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 6s 97ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n",
            "[0.0, 0.0]\n",
            "BinaryCrossentropyloss: 0.0\n",
            "Accuracy: 0.0\n"
          ]
        }
      ],
      "source": [
        "model_2.compile(metrics=[\"accuracy\"])\n",
        "evaluation = model_2.evaluate(test_ds)\n",
        "print(evaluation)\n",
        "\n",
        "print(f\"BinaryCrossentropyloss: {evaluation[0]}\")\n",
        "print(f\"Accuracy: {evaluation[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPsD3LyaMLHm"
      },
      "source": [
        "Note that categorical sets represent text differently from a dense embedding, so it may be useful to use both strategies jointly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37AGJamzboZQ"
      },
      "source": [
        "## Train a decision tree and neural network together\n",
        "\n",
        "The previous example used a pre-trained Neural Network (NN) to \n",
        "process the text features before passing them to the Random Forest. This example will train both the Neural Network and the Random Forest from scratch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJIxGwwzMkFl"
      },
      "source": [
        "TF-DF's Decision Forests do not back-propagate gradients ([although this is the subject of ongoing research](https://arxiv.org/abs/2007.14761)). Therefore, the training happens in two stages:\n",
        "\n",
        "1. Train the neural-network as a standard classification task:\n",
        "\n",
        "```\n",
        "example → [Normalize] → [Neural Network*] → [classification head] → prediction\n",
        "*: Training.\n",
        "```\n",
        "\n",
        "2. Replace the Neural Network's head (the last layer and the soft-max) with a Random Forest. Train the Random Forest as usual:\n",
        "\n",
        "```\n",
        "example → [Normalize] → [Neural Network] → [Random Forest*] → prediction\n",
        "*: Training.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSIvuAhzbjWO"
      },
      "source": [
        "### Prepare the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ore7f6tgcOMh"
      },
      "source": [
        "### Build the models\n",
        "\n",
        "Next create the neural network model using [Keras' functional style](https://www.tensorflow.org/guide/keras/functional). \n",
        "\n",
        "Bring back that Functional model from BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "S1Jfe4YteBqY"
      },
      "outputs": [],
      "source": [
        "input_1 = tf.keras.Input(shape=(1,), name=\"bill_length_mm\", dtype=\"float\")\n",
        "input_2 = tf.keras.Input(shape=(1,), name=\"island\", dtype=\"string\")\n",
        "\n",
        "nn_raw_inputs = [input_1, input_2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjlvAUNGeDM8"
      },
      "source": [
        "Use [preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) to convert the raw inputs to inputs apropriate for the neural netrwork. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9Q09Nkp6ei21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "56e43eda-97ab-48e7-9c1e-07a33ec1d666"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-b7c1fafad833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mStringLookup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringLookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ds_pd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bill_length_mm\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0minput_1_normalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0minput_1_normalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_ds_pd' is not defined"
          ]
        }
      ],
      "source": [
        "# Normalization.\n",
        "Normalization = tf.keras.layers.Normalization\n",
        "CategoryEncoding = tf.keras.layers.CategoryEncoding\n",
        "StringLookup = tf.keras.layers.StringLookup\n",
        "\n",
        "values = train_ds_pd[\"bill_length_mm\"].values[:, tf.newaxis]\n",
        "input_1_normalizer = Normalization()\n",
        "input_1_normalizer.adapt(values)\n",
        "\n",
        "values = train_ds_pd[\"island\"].values\n",
        "input_2_indexer = StringLookup(max_tokens=32)\n",
        "input_2_indexer.adapt(values)\n",
        "\n",
        "input_2_onehot = CategoryEncoding(output_mode=\"binary\", max_tokens=32)\n",
        "\n",
        "normalized_input_1 = input_1_normalizer(input_1)\n",
        "normalized_input_2 = input_2_onehot(input_2_indexer(input_2))\n",
        "\n",
        "nn_processed_inputs = [normalized_input_1, normalized_input_2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCoQljyhelau"
      },
      "source": [
        "Build the body of the neural network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzocgbYNsH6y"
      },
      "outputs": [],
      "source": [
        "y = tf.keras.layers.Concatenate()(nn_processed_inputs)\n",
        "y = tf.keras.layers.Dense(16, activation=tf.nn.relu6)(y)\n",
        "last_layer = tf.keras.layers.Dense(8, activation=tf.nn.relu, name=\"last\")(y)\n",
        "\n",
        "# \"3\" for the three label classes. If it were a binary classification, the\n",
        "# output dim would be 1.\n",
        "classification_output = tf.keras.layers.Dense(3)(y)\n",
        "\n",
        "nn_model = tf.keras.models.Model(nn_raw_inputs, classification_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPbRKf1CfIrj"
      },
      "source": [
        "This `nn_model` directly produces classification logits. \n",
        "\n",
        "Next create a decision forest model. This will operate on the high level features that the neural network extracts in the last layer before that classification head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fnpGNyTuXvH"
      },
      "outputs": [],
      "source": [
        "# To reduce the risk of mistakes, group both the decision forest and the\n",
        "# neural network in a single keras model.\n",
        "nn_without_head = tf.keras.models.Model(inputs=nn_model.inputs, outputs=last_layer)\n",
        "df_and_nn_model = tfdf.keras.RandomForestModel(preprocessing=nn_without_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trq07lvMudlz"
      },
      "source": [
        "### Train and evaluate the models\n",
        "\n",
        "The model will be trained in two stages. First train the neural network with its own classification head:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4OyUWKiupuF"
      },
      "outputs": [],
      "source": [
        "%set_cell_height 300\n",
        "\n",
        "nn_model.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(),\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=[\"accuracy\"])\n",
        "\n",
        "nn_model.fit(x=train_ds, validation_data=test_ds, epochs=10)\n",
        "nn_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2mgMZOpgMQp"
      },
      "source": [
        "The neural network layers are shared between the two models. So now that the neural network is trained the decision forest model will be fit to the trained output of the neural network layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAc9niXqud7V"
      },
      "outputs": [],
      "source": [
        "%set_cell_height 300\n",
        "\n",
        "df_and_nn_model.fit(x=train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF8Ru2HSv1a5"
      },
      "source": [
        "Now evaluate the composed model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPMlcObzuw89"
      },
      "outputs": [],
      "source": [
        "df_and_nn_model.compile(metrics=[\"accuracy\"])\n",
        "print(\"Evaluation:\", df_and_nn_model.evaluate(test_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awiHEznlv5sI"
      },
      "source": [
        "Compare it to the Neural Network alone:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--ompWYTvxM-"
      },
      "outputs": [],
      "source": [
        "print(\"Evaluation :\", nn_model.evaluate(test_ds))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "intermediate_colab.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}